<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://henriqueassumpcao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://henriqueassumpcao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-03T21:16:24+00:00</updated><id>https://henriqueassumpcao.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">finite fields</title><link href="https://henriqueassumpcao.github.io/blog/2024/finite_fields/" rel="alternate" type="text/html" title="finite fields"/><published>2024-10-10T00:00:00+00:00</published><updated>2024-10-10T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/finite_fields</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/finite_fields/"><![CDATA[<p>This post will contain some basic facts about finite fields. It will be quite useful for some future posts I plan to make on Galois theory, but these are facts that are useful in general for anyone interested in combinatorics and algebra.</p> <h2 id="finite-fields-and-multiplicative-groups">Finite fields and multiplicative groups</h2> <p>Let \(\mathbb{F}\) be a field, and define its <strong>characteristic</strong> \(\text{char}(\mathbb{F})\) as the smallest number \(n\) such that \(\sum_{i=1}^n 1 = 0\). If there is no such \(n\), we say that \(\text{char}(\mathbb{F}) = 0\). The first observation we must make is that \(\text{char}(\mathbb{F})\) is a prime number if \(\mathbb{F}\) is finite. Indeed, the characteristic of \(\mathbb{F}\) is certainly positive as the field is finite, and if it is a composite number \(n = ab\), then</p> \[(\sum_{i=1}^a 1)(\sum_{i=1}^b 1) = \sum_{i=1}^n 1 = 0,\] <p>hence either \(\sum_{i=1}^a 1 = 0\) or \(\sum_{i=1}^b 1 = 0\), and thus \(\text{char}(\mathbb{F})\) must indeed be prime. This will also imply that the size of \(\mathbb{F}\) must be a power of a prime. To see this, if \(\text{char}(\mathbb{F}) = p\) where \(p\) is prime, we consider the set \(\{1,2,...,p\} \subseteq \mathbb{F}\), hence we may identify a copy of \(\mathbb{Z}_p\) in \(\mathbb{F}\), and this copy is the <strong>prime subfield</strong> of \(\mathbb{F}\) – that is, the intersection of all subfields of \(\mathbb{F}\). This implies that \(\mathbb{F}\) can be seen as a vector space over \(\mathbb{Z}_p\), hence if \(a_1,...,a_k\) is a basis of this space, we have \(p^k\) possible linear combinations of these vectors, hence \(\vert\mathbb{F}\vert = p^k\). From this, we may always denote a finite field by \(\mathbb{F}_q\), where \(q\) is its size and \(q = p^k\), for some prime \(p\) and integer \(k\), with \(\text{char}(\mathbb{F}) = p\).</p> <p>If \(\mathbb{F}\) is a field, we denote by \(\mathbb{F}^*\) the set of invertible elements of the field – which in this case are all non-zero elements –, and it is immediate to note that this set is a group with multiplication, called the <strong>multiplicative group</strong>. We will soon see that the multiplicative group of a finite group is cyclic, but first we will lay out some basic facts about cyclic groups.</p> <p>All cyclic groups of a fixed order \(n\) are isomorphic, hence we denote by \(C_n\) the cyclic group of order \(n\). We first note that if \(g\) is a generator for \(C_n\), then ther order of any element \(g^i\) is given by</p> \[\text{ord}(g^i) = \frac{n}{\gcd(n,i)},\] <p>hence \(\text{ord}(g^i) = n\) iff \(\gcd(n,i) = 1\), and thus the number of generators of \(C_n\) is precisely given by \(\varphi(n)\), where</p> \[\varphi(n) = \{k \in \{1,...,n\}|\gcd(n,k) = 1\}\] <p>is the Euler totient function, i.e., \(\varphi(n) = \vert \mathbb{Z}_n^* \vert\). It is easy to see that any subgroup of \(C_n\) must also be cyclic, hence by Lagrange’s theorem it follows that any subgroup of \(C_n\) is a copy of \(C_d\), for some divisor \(d\) of \(n\). Now if \(d \vert n\), then the element \(g^{n/d}\) has order \(d\), hence its generated cyclic subgroup \(\langle g^{n/d} \rangle\) is a copy of \(C_d\). On the other hand, if \(g^i\) is some element in \(C_n\) of order \(d\), then \(n/d\) must be a divisor of \(i\), hence \(g^i \in \langle g^{n/d} \rangle\). This shows that for each divisor \(d\) of \(n\), there exists a unique subgroup \(C_d\) of \(C_n\), and we can express this subgroup as</p> \[C_d = \{x \in C_n|x^d = 1\}.\] <p>It is also worth noting that the previous observations imply that the set of elements of order \(d\) in \(C_n\) are all contained in the unique copy of \(C_d\) in \(C_n\), for some divisor \(d\), hence</p> \[\vert\{x \in C_n|\text{ord}(x) = d\}\vert = \vert\{x \in C_d|\text{ord}(x) = d\}\vert.\] <p>These facts imply the following useful equality:</p> \[\begin{split} n = |C_n| &amp;= \sum_{d \vert n} \vert\{x \in C_n|\text{ord}(x) = d\}\vert\\ &amp;= \sum_{d \vert n} \vert\{x \in C_d|\text{ord}(x) = d\}\vert\\ &amp;= \sum_{d \vert n} \varphi(d). \end{split}\] <p>We are now ready to prove the desired result.</p> <p><strong>Theorem 1:</strong> The multiplicative group \(\mathbb{F}_q^*\) of a finite field \(\mathbb{F}_q\) is cyclic.</p> <p><strong>Proof:</strong> Let \(n \geq 1\) be the size of \(\mathbb{F}_q^*\), and let \(m_d\) denote the number of elements of order \(d\) in \(\mathbb{F}_q^*\), and of course, if \(d\) doesn’t divide \(n\) then \(m_d = 0\). Our goal here is to show that \(m_n = \varphi(n) \geq 1\), and thus conclude that there is an element of order \(n\), which must necessarily be a generator for \(\mathbb{F}_q^*\).</p> <p>First, we note that any element of order \(d\) in our group must be a root of the polynomial \(t^d - 1\), but on the other hand, if \(x\) is an element of order \(d\), then all elements in the cyclic subgroup \(C_d\) it generates are also roots of \(t^d - 1\). Since this polynomial has at most \(d\) roots in \(\mathbb{F}_q\), it follows that \(C_d\) is precisely this set of roots, that is,</p> \[C_d = \{x \in \mathbb{F}_q|x^d = 1\}.\] <p>This shows that if \(m_d &gt; 0\), then \(m_d = \varphi(d)\), since in this case all of the \(m_d\) elements of order \(d\) in \(\mathbb{F}_q\) would be precisely the \(\varphi(d)\) generators of \(C_d\). Hence, for any \(d\) that divides \(n\), it follows that \(m_d \leq \varphi(d)\). We thus obtain the following:</p> \[n = |\mathbb{F}_q^*| = \sum_{d \vert n}m_d \leq \sum_{d \vert n}\varphi(d) = n,\] <p>which implies that \(m_d = \varphi(d)\), and in particular, \(m_n = \varphi(n) \geq 1\), which concludes the proof.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="some-facts-about-field-extensions">Some facts about field extensions</h2> <p>Before moving forward, we’ll need some results about field extensions. If \(\mathbb{E}\) is a field that contains a field \(\mathbb{F}\), we say that \(\mathbb{E}:\mathbb{F}\) is a <strong>field extension</strong>, and the <strong>degree</strong> of this extension is the dimension of \(\mathbb{E}\) as a vector space over \(\mathbb{F}\). We then note the following:</p> <p><strong>Lemma 1:</strong> Let \(\mathbb{F}\) be a field, \(f(x) \in \mathbb{F}[x]\) a polynomial with coefficients on the field. Then the following hold:</p> <ul> <li> <p>If \(f'(x)\) denotes the formal derivative of \(f\), then \(\gcd(f,f') \neq 1\) iff \(f\) has multiple roots in some extension of \(\mathbb{F}\);</p> </li> <li> <p>If \(g \in \mathbb{F}[x]\), then \(\gcd(f,g)\) is invariant w.r.t. extensions of \(\mathbb{F}\).</p> </li> </ul> <p><strong>Proof:</strong> For the first claim, if \(d = \gcd(f,f') \neq 1\), then it has degree at least \(1\), which implies that there is some root \(\alpha\) of \(d\) is some extension \(\mathbb{E}\) of \(\mathbb{F}\). Hence, we have that \((x-\alpha)\) divides \(f\), and thus</p> \[f = q(x-\alpha) \Rightarrow f' = q'(x-\alpha) + q,\] <p>thus \(q = f-q'(x-\alpha) = (x-\alpha)(q-q')\), thus \(x-\alpha\) divides \(q\), which implies that \((x-\alpha)^2\) divides \(f\), hence it has multiple roots. Now if \((x-\alpha)^2\) divides \(f\) for some \(\alpha\) in an extension of \(\mathbb{F}\), then</p> \[f = q(x-\alpha)^2 \Rightarrow f' = q'(x-\alpha)^2 + 2q(x-\alpha),\] <p>thus \((x-\alpha)\) divides \(f'\), hence \(\gcd(f,f') \neq 1\).</p> <p>For the second claim, if \(d\) is the gcd of \(f,g\) in \(\mathbb{F}\), and \(D\) is the gcd in \(\mathbb{E}\), then we know that there are polynomials \(r,s \in \mathbb{F}[x]\) such that</p> \[d = rf + sg,\] <p>and since \(D\) divides both \(r\) and \(s\), it follows that \(D\) divides \(d\), but on the other hand, \(D\) is defined as the gcd over \(\mathbb{E}\), hence any polynomial that divides both \(f,g\) must divide \(D\), and so \(D = d\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>If \(f(x)\) is a polynomial over \(\mathbb{F}\), and if \(\mathbb{E}\) is an extension such that we can write</p> \[f(x) = \alpha_0(x-\alpha_1)\ldots(x-\alpha_n),\] <p>where \(n = \text{deg}(f)\) and \(\alpha_i \in \mathbb{E}\), then we say that the smallest extension of \(\mathbb{F}\) containing each \(\alpha_i\) is a <strong>splitting field</strong> of \(f\). We will discuss such fields in more detail in a later post, but for now we only need to know that we can decompose \(f\) as a product of irreducibles in the splitting field, that is, such field contains all roots of our polynomial.</p> <h2 id="fundamental-theorem-of-finite-fields">Fundamental Theorem of Finite Fields</h2> <p>We can now prove the basic theorem about finit fields: we can always create a finite field of order \(p^k\) for any fixed prime \(p\) and integer \(k\), and all finite fields of the same order are isomorphic. For this reason, it is usual to see authors denote a finite field of order \(q\) simply by \(\mathbb{F}_q\) or \(\text{GF}(q)\), as they are unique up to isomorphism.</p> <p>First, we introduce a rather useful function of finite fields. Define</p> <p>\(\begin{split} \Phi:\mathbb{F}_q &amp;\mapsto \mathbb{F}_q,\\ \Phi(x) &amp;= x^p, \end{split}\) where \(p\) is the characteristic of our finite field \(\mathbb{F}_q\). Our claim is that this map is a field automorphism. Indeed, we first note that \(\Phi(xy) = \Phi(x)\Phi(y),\Phi(1) = 1\) and that</p> \[\Phi(x+y) = x^p + (\sum_{k=1}^{p-1}{p \choose k}x^ky^{p-k}) + y^p,\] <p>but since \(p\) is prime, the coefficients \({p \choose k}\) are all multiples of \(p\), hence \(\Phi(x+y) = \Phi(x)+\Phi(y)\), and so \(\Phi\) is a field homomorphism. It is certainly injective, since if \(x^p = 1\) then \(x = 1\). Since \(\mathbb{F}_q\) has finite size \(p^k\), its multiplicative group has size \(q-1\) and so \(x^q = x\) for any \(x \in \mathbb{F}_q\), hence \(\Phi(x^{p^{k-1}}) = x\), and thus \(\Phi\) is surjective.</p> <p>With this, we can prove the following:</p> <p><strong>Theorem 2:</strong> Let \(p\) be a prime number, and let \(k\) be some nonnegative integer, with \(q = p^k\). Then the following hold:</p> <ul> <li>There exists some finite field of size \(q\);</li> <li>Every finite field with \(q\) elements is a splitting field for the polynomial \(t^q - t\);</li> </ul> <p><strong>Proof:</strong> For the first claim, we let \(\mathbb{K}\) be a splitting field of \(f(t) = t^q - t\) over \(\mathbb{Z}_p\), and we consider the set of all roots of this polynomial:</p> \[\mathbb{F} = \{\alpha \in \mathbb{K}|f(\alpha) = 0\}.\] <p>This set has size at most \(q\), however note that</p> \[(f)' = qt^{q-1} - 1 = 1,\] <p>since \(q = p^k = 0\) in \(\mathbb{Z}_p\), hence \(\gcd(f,f') = 1\), which shows that \(f\) has no multiple roots, hence \(\mathbb{F}\) has size \(q\). It is easy to check that \(\mathbb{F}\) is indeed a field, which proves the claim.</p> <p>For the second claim, let \(\mathbb{K}\) be field of size \(q\), and note for any element \(\alpha\) in the field, it follows that \(\alpha^{q-1} = 1\), hence if we write</p> \[f(t) = t^q - t = t(t^{q-1} - 1),\] <p>we get that \(f(\alpha) = 0\), thus all elements of \(\mathbb{K}\) are roots of \(f\), and so \(\mathbb{K}\) is a splitting field.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>We can also prove that if \(\mathbb{F}_1,\mathbb{F}_2\) are fields of size \(q\), then they are isomorphic, but in order to do this we need some results about splitting fields that I will introduce in a later post.</p> <p>To end this post, we prove the following:</p> <p><strong>Proposition 1:</strong> If \(\mathbb{F}_q\) is a finite field, with \(q = p^k\) for some prime \(p\), then the subfields of \(\mathbb{F}_q\) are uniquely determined by the divisors of \(k\), i.e., \(\mathbb{K} \subseteq \mathbb{F}_q\) is a subfield if and only if \(\vert\mathbb{K}\vert = p^e\) and \(e \vert d\), and these subfields are unique.</p> <p><strong>Proof:</strong> First, we note that since any subfield of \(\mathbb{F}_q\) is also an abelian subgroup, it follows that its order must be some power of \(p\) – simply by Lagrange’s Theorem. Now if \(\mathbb{K}\) is a subfield of \(\mathbb{F}_q\) of order \(p^d\), then \(\mathbb{K}^* \leq \mathbb{F}_q^*\), hence again by Lagrange’s Theorem it follows that</p> \[p^d - 1 = \vert \mathbb{K}^*\vert\vert \vert \mathbb{F}_q^* \vert = p^k - 1\] <p>and since \(p\) is a prime this implies that \(d \vert k\). Now assume that \(d \vert k\), and thus \(p^d - 1 \vert p^k - 1\), since \(\mathbb{F}_q^*\) is a cyclic group, there exists a unique subgroup \(H\) of order \(p^d - 1\), where of course \(H\) is also cyclic, and since \(H\) can be identified as</p> \[H = \{x \in \mathbb{F}_q^*|x^{p^d} = x\}\] <p>it follows that \(\mathbb{K} = \{0\} \cup H\) is a subfield of \(\mathbb{F}_q\) with order \(p^k\), where \(d \vert k\), and the uniqueness of the subgroup implies that \(\mathbb{K}\) is also unique. The facts together show that there are unique subfields of \(\mathbb{F}_q\) for every \(d \vert k\), and these are all of the possible subfields.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\]]]></content><author><name></name></author><category term="math"/><category term="galois-theory"/><summary type="html"><![CDATA[some basic facts about finite fields]]></summary></entry><entry><title type="html">simplicity of the alternating group</title><link href="https://henriqueassumpcao.github.io/blog/2024/simplicity_alternating/" rel="alternate" type="text/html" title="simplicity of the alternating group"/><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/simplicity_alternating</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/simplicity_alternating/"><![CDATA[<p>This post has a few objectives: prove that the alternating group \(A_n\) is simple, and then conclude that \(S_n\) is not solvable, both for when \(n \geq 5\). In order to do this, we’ll first discuss some facts about \(2\)-transitive groups, and then see a few examples when \(n &lt; 5\). After this post, I’ll hopefully start to talk about some basic results of Galois Theory (as soon as I figure out a good way of plotting tikzcd diagrams on this blog).</p> <h2 id="transitive-groups">Transitive groups</h2> <p>If \(G\) is a group that acts on a set \(X\), i.e., if there is a homomorphism from \(G\) to \(\text{Sym}(X)\), then we say that \(G\) acts transitively on \(X\) if for every \(\alpha,\beta \in X\), there exists some \(g \in G\) such that \(\alpha g = \beta\). In other words, for every element \(\alpha \in G\), its orbit \(\alpha G\) is the entire set \(X\). A group is said to be \(2\)-transitive on \(X\) if, for every \(\alpha,\beta,\gamma,\delta \in X,\alpha \neq \beta, \gamma \neq \delta\), there exists some \(g \in G\) such that</p> \[(\alpha,\beta)g = (\alpha g,\beta g) = (\gamma,\delta)\] <p>Equivalently, a group is \(2\)-transitive on \(X\) if it is transitive on the set of distinct ordered pairs \(\{(\alpha,\beta)\vert \alpha,\beta \in X,\alpha \neq \beta\}\). From this definition, it follows that any \(2\)-transitive group is also transitive, and we can also prove the following characterization.</p> <p><strong>Proposition 1:</strong> A group \(G\) is \(2\)-transitive on \(X\) if, and only if, the stabilizer \(G_\alpha\) of any element \(\alpha \in X\) acts transitively on \(X\setminus\{\alpha\}\).</p> <p><strong>Proof:</strong> First, assume that \(G\) is \(2\)-transitive, and let \(G_\alpha\) be the stabilizer of some \(\alpha \in G\). By definition, we know that for any \(\beta,\delta \in X\setminus\{\alpha\}\), there exists some \(g \in G\) such that</p> \[(\alpha,\beta)g = (\alpha,\delta)\] <p>hence \(\beta g = \delta\) and \(\alpha g = \alpha\), and thus \(g \in G_\alpha\), implying that this stabilizer indeed acts transitively on \(X\setminus\{\alpha\}\).</p> <p>Now if \(\alpha,\beta,\gamma,\delta \in X\), with \(\alpha \neq \beta, \gamma \neq \delta\), we can find elements \(g_1,g_2 \in G\) such that</p> \[\begin{split} (\alpha,\beta)g_1 &amp;= (\alpha,\delta)\\ (\alpha,\delta)g_2 &amp;= (\gamma,\delta) \end{split}\] <p>by simply using the transitivity of \(G_\alpha,G_\delta\), hence</p> \[(\alpha,\beta)g_1g_2 = (\gamma,\delta)\] <p>and thus \(G\) is \(2\)-transitive, as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>Using this result, we can prove an auxiliary lemma that will be quite useful.</p> <p><strong>Lemma 1:</strong> If \(G \neq 1\) is a \(2\)-transitive group acting on \(X\), then the stabilizer \(G_\alpha\) of any element \(\alpha \in X\) is a maximal subgroup of \(G\).</p> <p><strong>Proof:</strong> Let \(g \in G \setminus G_\alpha\) be an element of the group that is not in the stabilizer, i.e., \(\alpha g = \beta \neq \alpha\), and let \(H = \langle G_\alpha,g \rangle\). We will show that \(H = G\), and since this will be true for any \(g \in G \setminus G_\alpha\), it will follow that \(G_\alpha\) is maximal. We know that the lateral classes of \(G_\alpha\) partition \(G\), hence let \(h \in G\) be an element of the group, and consider the lateral class \(G_\alpha h\). Let \(\beta h = \gamma\), and note that since \(G_\alpha\) is transitive on \(X \setminus \{\alpha\}\), we can find some \(g_\gamma \in G_\alpha\) s.t. \(\beta g_\gamma = \gamma\), hence \(\alpha gg_\gamma = \beta\). We claim that \(G_\alpha gg_\gamma = G_\alpha h\). Indeed, note that</p> \[\begin{split} (fgg_\gamma h^{-1})h &amp;\in G_\alpha h\\ (fhg_\gamma^{-1}g^{-1})gg_\gamma &amp;\in G_\alpha gg_\gamma \end{split}\] <p>for any \(f \in G_\alpha\), and thus the claim follows. This shows that any lateral class of \(G_\alpha\) is contained in \(H\), implying that \(G \subseteq H\), and hence \(H = G\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>We can use the previous Lemma to show that \(S_n\) is a maximal subgroup of \(S_{n+1}\), and similarly that \(A_n\) is a maximal subgroup of \(A_{n+1}\). First, note that \(S_n\) acts transitively on \(\{1,...,n\}\), since we can take any element \(i\) to some other element \(j\) via the transposition \((i,j)\), and similarly \(A_n\) is also transitive since we can take \(i\) to \(j\) via \((i,k)(k,j)\). Now if we consider any fixed element \(i \in \{1,...,n+1\}\), then the stabilizer of this element in \(S_{n+1}\) is a copy of \(S_n\), hence it is transitive on \(\{1,...,n+1\} \setminus \{i\}\), implying that \(S_{n+1}\) is \(2\)-transitive, and thus \(S_n \leq S_{n+1}\) is maximal. A similar argument shows that \(A_n\) is maximal in \(A_{n+1}\).</p> <h2 id="conjugacy-classes-and-simplicity">Conjugacy classes and simplicity</h2> <p>We are now ready to prove that \(A_n\) is simple if \(n \geq 5\). The strategy will be as follows: we start with showing that \(A_5\) is simple, by analyzing its conjugacy classes and finding a contradiction with the existence of a normal proper non-trivial subgroup. We then proceed inductively on \(n\), and use the previous results about transitivity to complete the proof.</p> <p>First, <a href="https://henriqueassumpcao.github.io/blog/2024/symmetric_group/">recall from the previous post</a> that the conjugacy classes of \(S_n\) are completely determined by the integer partitions of \(n\), and in turn we can completely determine the conjugacy classes of \(A_n\) by analyzing the centralizers of these classes. Below is a table of the conjugacy classes of \(S_5\), where each row is indexed by the respective partition, \(x\) denotes a representative of the conjugacy class, \(C_{S_5}(x)\) denotes its centralizer, and \(x^{S_5}\) denotes its orbit.</p> <table> <thead> <tr> <th style="text-align: left">Partition</th> <th style="text-align: center">\(x\)</th> <th style="text-align: center">\(C_{S_5}(x)\)</th> <th style="text-align: right">\(\vert x^{S_5}\vert\)</th> <th style="text-align: right">\(\vert C_{S_5}(x) \vert\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\((1,1,1,1,1)\)</td> <td style="text-align: center">\(1\)</td> <td style="text-align: center">\(S_5\)</td> <td style="text-align: right">\(1\)</td> <td style="text-align: right">\(120\)</td> </tr> <tr> <td style="text-align: left">\((2,1,1,1)\)</td> <td style="text-align: center">\((1,2)\)</td> <td style="text-align: center">\(\langle (1,2),(3,4),(3,4,5)\rangle\)</td> <td style="text-align: right">\(10\)</td> <td style="text-align: right">\(12\)</td> </tr> <tr> <td style="text-align: left">\((2,2,1)\)</td> <td style="text-align: center">\((1,2)(3,4)\)</td> <td style="text-align: center">\(\langle (1,2),(1,3,2,4)\rangle\)</td> <td style="text-align: right">\(15\)</td> <td style="text-align: right">\(8\)</td> </tr> <tr> <td style="text-align: left">\((3,1,1)\)</td> <td style="text-align: center">\((1,2,3)\)</td> <td style="text-align: center">\(\langle (1,2,3),(4,5)\rangle\)</td> <td style="text-align: right">\(20\)</td> <td style="text-align: right">\(6\)</td> </tr> <tr> <td style="text-align: left">\((3,2)\)</td> <td style="text-align: center">\((1,2,3)(4,5)\)</td> <td style="text-align: center">\(\langle (1,2,3),(4,5)\rangle\)</td> <td style="text-align: right">\(20\)</td> <td style="text-align: right">\(6\)</td> </tr> <tr> <td style="text-align: left">\((4,1)\)</td> <td style="text-align: center">\((1,2,3,4)\)</td> <td style="text-align: center">\(\langle (1,2,3,4)\rangle\)</td> <td style="text-align: right">\(30\)</td> <td style="text-align: right">\(4\)</td> </tr> <tr> <td style="text-align: left">\((5)\)</td> <td style="text-align: center">\((1,2,3,4,5)\)</td> <td style="text-align: center">\(\langle (1,2,3,4,5)\rangle\)</td> <td style="text-align: right">\(24\)</td> <td style="text-align: right">\(5\)</td> </tr> </tbody> </table> <p>Now recall that \(A_5\) contains only even permutations, hence the conjugacy classes corresponding to the partitions \((2,1,1,1),(3,2),(4,1)\) will have empty intersections with \(A_5\). The centralizer of the class corresponding to \((2,2,1)\) is not contained in \(A_5\), and thus the conjugacy class is also a class in \(A_5\), and the same is true for \((3,1,1)\). The centralizer of \((5)\) on the other hand is contained in \(A_5\), thus the class splits into two disjoint conjugacy classes in \(A_5\). With these observations, we obtain the following table of the conjugacy classes of \(A_5\):</p> <table> <thead> <tr> <th style="text-align: center">\(x\)</th> <th style="text-align: center">\(C_{A_5}(x)\)</th> <th style="text-align: right">\(\vert x^{A_5}\vert\)</th> <th style="text-align: right">\(\vert C_{A_5}(x) \vert\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(1\)</td> <td style="text-align: center">\(A_5\)</td> <td style="text-align: right">\(1\)</td> <td style="text-align: right">\(60\)</td> </tr> <tr> <td style="text-align: center">\((1,2)(3,4)\)</td> <td style="text-align: center">\(\langle (1,2)(3,4),(1,3)(2,4)\rangle\)</td> <td style="text-align: right">\(15\)</td> <td style="text-align: right">\(4\)</td> </tr> <tr> <td style="text-align: center">\((1,2,3)\)</td> <td style="text-align: center">\(\langle (1,2,3)\rangle\)</td> <td style="text-align: right">\(20\)</td> <td style="text-align: right">\(3\)</td> </tr> <tr> <td style="text-align: center">\((1,2,3,4,5)\)</td> <td style="text-align: center">\(\langle (1,2,3,4,5)\rangle\)</td> <td style="text-align: right">\(12\)</td> <td style="text-align: right">\(5\)</td> </tr> <tr> <td style="text-align: center">\((1,2,3,5,4)\)</td> <td style="text-align: center">\(\langle (1,2,3,5,4)\rangle\)</td> <td style="text-align: right">\(12\)</td> <td style="text-align: right">\(5\)</td> </tr> </tbody> </table> <p>Now assume that \(N \trianglelefteq A_5\) is a normal subgroup, thus it must be the disjoint union of conjugacy classes in \(A_5\), hence \(\vert N \vert \mid 60\), and on the other hand there must be integers \(x_1,x_2,x_3\) such that</p> \[\vert N \vert = 15x_1 + 20x_2 + 12x_3\] <p>where \(x_1,x_2 \in \{0,1\},x_3 \in \{0,1,2\}\) correspond to the possible choices of conjugacy classes. These two facts together with a simple inspection show that either \(N = A_5\) or \(N = 1\), and thus \(A_5\) is indeed simple. This will serve as the base case of our induction in the following theorem.</p> <p><strong>Theorem 1:</strong> The alternating group \(A_n\) is simple for \(n \geq 5\).</p> <p><strong>Proof:</strong> We proceed via induction on \(n\). The base case of \(n = 5\) was already verified, so assume that the claim holds for values less than \(n+1\) and consider \(A_{n+1}\). Let \(i \in \{1,2,...,n+1\}\), and note that the stabilizer \(G_i\) is isomorphic to \(A_n\), and since \(A_{n+1}\) is transitive, it follows that all \(G_i\)’s are conjugate.</p> <p>Let \(N \trianglelefteq A_{n+1}\) be a normal subgroup, and consider \(N \cap G_1 \trianglelefteq G_1\), and since \(G_1\) is isomorphic to \(A_n\), we know by the inductive hypothesis that either \(N \cap G_1 = G_1\) or \(N \cap G_1 = 1\). If \(N \cap G_1 = G_1\), since the \(G_i\)’s are conjugate and \(N\) is normal, it follows that \(G_i \subseteq N\) for all \(i\). However, note that \(N\) is normal but \(G_1\) isn’t, hence \(N\) properly contains each \(G_1\), and since each \(G_i\) is a maximal subgroup of \(A_{n+1}\), this implies that \(N = A_{n+1}\). Now assume that for all \(i\), we have \(N \cap G_i = 1\), and assume that \(N \neq 1\). Since \(G_1\) is maximal, it follows that \(NG_1 = A_{n+1}\), hence</p> \[\vert NG_1 \vert = \frac{\vert N \vert\vert G_1 \vert}{\vert N \cap G_1 \vert} = \frac{\vert N \vert}{\vert A_n\vert}\] <p>hence \(\vert N \vert = n+1\). Let \(\sigma \in N \setminus\{1\}\), and write its decomposition as a product of disjoint cycles as follows:</p> \[\sigma = \sigma_1\ldots\sigma_k\] <p>where each cycle has length \(c_i\), with \(c_1 \geq \ldots \geq c_k\). If \(c_1 \geq 3\), we may assume WLOG that \(\sigma_1 = (1,2,3,...,m)\), for some \(m \geq 3\), thus if we take \(\rho = (3,4,5)\), we get that \(\sigma^\rho\sigma^{-1}\) is an element of \(G_1 \cap N \setminus \{1\}\), which is a contradiction. If \(c_1 = 2\), then we may assume WLOG that \(\sigma_1 = (1,2)(3,4)\), and thus if we take \(\rho = (4,5,6)\) we again get that \(\sigma^\rho\sigma^{-1}\) is an element of \(G_1 \cap N \setminus \{1\}\), which is also a contradiction. This implies that \(A_{n+1}\) is simple, as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="simplicity-and-solvability">Simplicity and solvability</h2> <p>The simplicity of \(A_n\) for \(n \geq 5\) also implies that \(A_n\) is the only proper non-trivial normal subgroup of \(S_n\) in these cases.</p> <p><strong>Theorem 2:</strong> If \(n \geq 5\), then \(A_n\) is the only proper non-trivial normal subgroup of \(S_n\).</p> <p><strong>Proof:</strong> Suppose that \(N \trianglelefteq S_n\) is a proper normal subgroup of \(S_n\) that is not equal to \(1\). Since \(A_n\) is also a normal subgroup, it follows that \(A_n \cap N\) is also normal and proper in \(S_n\). If \(A_n \cap N = 1\), then \(N\) must be composed only of odd permutations, however the product of two odd permutations is even, and thus \(A_n \cap N \neq 1\), i.e., \(A_n \cap N\) is a non-trivial normal subgroup of both \(A_n\) and \(N\), but since \(A_n\) is simple if \(n \geq 5\), it follows that \(A_n \cap N = A_n\), and thus \(A_n \subseteq N\). On the other hand, \(A_n\) is a maximal subgroup of \(S_n\), thus it follows that \(N = A_n\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>Now it is actually fairly straightforward to check that \(S_n\) is not solvable for \(n \geq 5\). Recall that a group \(G\) is solvable iff there exists some normal series</p> \[G = G_0 \trianglerighteq G_1 \ldots \trianglerighteq G_k = 1\] <p>where each \(G_i \trianglelefteq G\), and \(G_i/G_{i+1}\) is abelian. If \(n \geq 5\), since neither \(S_n\) nor \(A_n\) are abelian, and since the only normal subgroup of \(S_n\) is \(A_n\), any normal series is of the form</p> \[S_n \trianglerighteq A_n \trianglerighteq 1\] <p>hence the series does not satisfy the solvability conditions, implying that \(S_n\) is not solvable if \(n \geq 5\).</p> <p>On the other hand, the groups \(S_1,S_2,S_3,S_4\) are all solvable. This is immediately true for \(S_1\) and \(S_2\), as they are abelian. The solvability of \(S_3\) follows from a more general fact: the dihedral group \(D_n\) is actually solvable for all \(n\), since if we consider the subgroup of all rotations \(C_n\), it has order \(n\), and since \(D_n\) has order \(2n\) it follows that the index of \(C_n\) is two, and thus it is normal, hence the series</p> \[D_n \trianglerighteq C_n \trianglerighteq 1\] <p>is normal, and \(D_n/C_n\) is abelian as it is a group of order two, and since \(S_3 = D_3\), the result follows. Now in order to see that \(S_4\) is solvable, it will require some more effort. We first consider the set</p> \[\begin{split} X &amp;= \{X_1,X_2,X_3\},\\ X_1 &amp;= \{\{1,2\},\{3,4\}\},\\ X_2 &amp;= \{\{1,3\},\{2,4\}\},\\ X_3 &amp;= \{\{1,4\},\{2,3\}\} \end{split}\] <p>and note that \(S_4\) acts on \(X\) as follows by permuting the elements of the sets, e.g.,</p> \[\begin{split} \{1,2\}(1,2,3) &amp;= \{1(1,2,3),2(1,2,3)\} = \{2,3\}\\ X_1(1,2,3) &amp;= X_3 \end{split}\] <p>Since an action is nothing more than an homomorphism from the group to the group of symmetries of the set, it follows that we obtain an homomorphism \(\varphi:S_4 \mapsto S_3\), since \(\text{Sym}(X) = S_3\). It follows by a simple computation that</p> \[\text{ker}(\varphi) = \{1,(1,2)(3,4),(1,3)(2,4),(1,4)(2,3)\}\] <p>and the set \(\text{ker}(\varphi)\) is often called the <em>Klein group</em> \(K_4\) on \(4\) elements. From this, we have that \(S_4/K_4\) is isomorphic to some subgroup of \(S_3\), but on the other hand \(S_4/K_4\) has order \(6\), thus</p> \[S_4/K_4 \cong S_3\] <p>Now note that \(K_4\) has \(4\) elements, and thus it is abelian, and in particular it is solvable, and since it is the kernel of an homomorphism it follows that \(K_4\) is a normal solvable subgroup of \(S_4\). <a href="https://henriqueassumpcao.github.io/blog/2024/solvable_groups/">Recall</a> that if \(N\) is a normal solvable subgroup of \(G\), and if \(G/N\) is solvable, then \(G\) is also solvable, thus this implies that \(S_4\) is solvable.</p>]]></content><author><name></name></author><category term="math"/><category term="group-theory"/><summary type="html"><![CDATA[proof that the alternating group is simple on 5 or more elements, and some interesting consequences]]></summary></entry><entry><title type="html">symmetric and alternating groups</title><link href="https://henriqueassumpcao.github.io/blog/2024/symmetric_group/" rel="alternate" type="text/html" title="symmetric and alternating groups"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/symmetric_group</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/symmetric_group/"><![CDATA[<p>This is the first amongst two posts about the symmetric group and the alternating group, which in turn are part of a series of posts that will hopefully culminate in a self-contained proof of the fundamental theorem of Galois Theory. Here I will discuss some basic facts, and set the stage for the proofs regarding the simplicity of the alternating group and the solvability of the symmetric group.</p> <h2 id="basic-facts-about-the-symmetric-group">Basic facts about the symmetric group</h2> <p>If \(X\) is a set, we denote by \(\text{Sym}(X)\) as the set of all bijections from \(X\) to \(X\), and this set together with the operation of function composition forms a group called the <em>symmetric group</em> of \(X\). When \(X\) is a finite set, say with \(n\) elements, we denote this group simply by \(\text{Sym}(n)\), or more commonly by \(S_n\), and we usually omit the reference to \(X\). The symmetric group is extremely relevant in group theory, and its structure implies many different nontrivial results from many differents areas in mathematics.</p> <p>When working with elements of the symmetric group, the cyclic notation for permutations is quite useful. We denote by \((i_1,i_2)\) the permutations – which is often called a transposition – that simply maps \(i_1\) to \(i_2\) and vice-versa, i.e., it simply switches \(i_1\) with \(i_2\), and by analogy we can also consider \((i_1,...,i_k)\) and the permutation – called a cycle – that maps \(i_1\) to \(i_{2}\), and so on, noting that \(i_k\) gets mapped to \(i_1\). Two cycles are called disjoint if they don’t permute any common elements, and it is immediate to check that two disjoint permutations commute, and also that the order of a given cycle \(\sigma = (i_1,...,i_k)\), i.e., the minimum exponent \(l\) such that \(\sigma^l = 1\), is precisely equal to its length \(k\).</p> <p>We highlight that, when working with permutations, we denote the image of an element \(x\) by a permutation \(\sigma\) as \(x\sigma\), i.e., we apply the permutations on the right-hand side. We now show that all elements in \(S_n\) can be expressed in terms of cycles and permutations.</p> <p><strong>Proposition 1:</strong> If \(\sigma \in S_n\) is a permutation, then the following hold:</p> <ul> <li>The permutation can be decomposed as a product of disjoint cycles \(\sigma = \sigma_1\cdot\ldots\sigma_k\), and the decomposition is unique up to the order of the cycles.</li> <li>The permutation can be decomposed as a product of transpositions \(\sigma = \tau_1 \cdot \ldots \tau_l\), and the parity of the number of such transpositions is uniquely determined by \(\sigma\), i.e., if \(\sigma = \tau'_1 \cdot \ldots \tau'_r\), then \(l\) and \(r\) have the same parity.</li> </ul> <p><strong>Proof:</strong></p> <p>For the first item, we can proceed inductively: start with an element \(i \in \{1,...,n\}\) that is not fixed by \(\sigma\), and consider the set \(\{i,i\sigma,i\sigma^2,...\}\). This set must be finite, and thus we obtain a cycle \((i,i\sigma,...,i\sigma^{k_i-1})\) where \(i\sigma^{i_k} = i\), and hence we can consider the permutation \(\sigma'\) that is obtained from sigma by simply fixing all elements in the aforementioned cycle, i.e., we obtain a cycle that fixes more elements than \(\sigma\). We can repeat this process inductively, and since our base set is finite, we eventually obtain the trivial permutation, and if we consider the product of all cycles obtained in this process we get the desired decomposition. Another way of visualizing this is by simply considering the directed graph on the set \(\{1,...,n\}\) induced by \(\sigma\), where two nodes \(i,j\) are connected via a directed edge if \(i\sigma = j\), and hence we are simply finding the connected components of this graph – and since each note has only one outgoing and incoming edges, these components are cycles. From this, uniqueness also follows immediately.</p> <p>Now for the second item, we can use the previous item to see that we only need to prove this result for cycles. Let \(\sigma = (i_1,...,i_k)\) be a cycle, and thus we can simply consider the product \((i_1,i_2)(i_1,i_3)\ldots(i_1,i_k)\) of \(k-1\) transpositions, and note that this product is precisely equal to \(\sigma\). We now need to check that the parity of this decomposition is in fact unique, and in order for us to do this we’ll have to consider the action of \(S_n\) on the polynomial ring \(\mathbb{Q}[x_1,...,x_n]\) on \(n\) variables, where the action simply permutes the indices of the variables. This is clearly a faithful action, i.e., if a permutation fixes all indices, then it must be the trivial permutation. We can now consider the following polynomial:</p> \[F = \prod_{1\leq i&lt; j \leq n}(x_i - x_j)\] <p>Note that if \(\tau = (i,j)\) is a transposition, then \(\tau(F) = -F\), hence if \(\sigma = \tau_1\ldots\tau_k=\tau'_1\ldots\tau'_l\) is written as two products of transpositions, then \(\sigma(F) = (-1)^kF = (-1)^lF\), and thus \(l\) and \(k\) must have the same parity.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>If \(\sigma = (i_1,...,i_k)\) is a cycle, and if \(\pi \in S_n\) is an arbitrary permutation, we can obtain a nice expression for the conjugation of \(\sigma\) by \(\pi\):</p> \[\sigma^\pi = (i_1\pi,...,i_k\pi)\] <p>Indeed, this follows by simply noting that if \(j \in \{1,...,n\}\), then either:</p> <ul> <li>The permutation \(\pi^{-1}\) maps \(j\) to some \(i_l\) in \(\{i_1,...,i_k\}\), and in this case we get that \(j\sigma^\pi = i_{l+1}\pi\).</li> <li>If \(\pi^{-1}\) does not belong to \(\{i_1,...,i_k\}\), then \(j\sigma^\pi = j\).</li> </ul> <p>From these observations, the claim follows. We can now prove that \(S_n\) is generated by some subsets of permutations that will be rather useful in future results.</p> <p><strong>Proposition 2:</strong> The symmetric group \(S_n\) on \(n\) elements can be written as:</p> \[\begin{split} S_n &amp;= \langle\{(i,j)|1 \leq i,j \leq n\}\rangle\\ &amp;= \langle\{(1,2),(2,3),...,(n-1,n)\}\rangle\\ &amp;= \langle\{(1,2),(1,2,\ldots,n)\}\rangle \end{split}\] <p><strong>Proof:</strong></p> <p>The first equation simply follows from the previous proposition, as any element can be written as a product of transpositions.</p> <p>For the second equation, note that if \(\tau = (i,j)\) is a transposition, with \(i &lt; j\), then</p> \[(1,2)^{(2,3)(3,4)...(j-1,j)} = (1,j)\] <p>and analogously</p> \[(1,j)^{(1,2)(2,3)...(i-1,i)} = (i,j)\] <p>hence the set \(\langle\{(1,2),(2,3),...,(n-1,n)\}\rangle\) contains all transpositions, and thus it must be equal to \(S_n\).</p> <p>For the third equation, we let \(\sigma = (1,2,...,n),\tau = (1,2)\) and \(\sigma^{-1} = (1,n,n-1,...,3,2)\), and we note that given a transposition \((k,k+1)\), we have</p> \[(k,k+1) = \sigma^{-(k-1)}\tau\sigma^{k-1}\] <p>hence the set \(\langle\{(1,2),(1,2,\ldots,n)\}\rangle\) contains all elements of \(\langle\{(1,2),(2,3),...,(n-1,n)\}\rangle\), and thus by the previous item it must be equal to \(S_n\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>The symmetric group also has a pretty interesting structure when it comes to its action on itself by conjugation. If we consider a permutation \(\sigma,\sigma \in S_n\), and look at the orbit of \(\sigma\) w.r.t. conjugation by elements of \(S_n\), i.e., the set</p> \[\sigma^{S_n} = \{\sigma^\pi|\pi \in S_n\}\] <p>which is often called the <em>conjugacy class</em> of \(\sigma\) in \(S_n\), and we can actually characterize this set in terms of \(\sigma\). We know that \(\sigma\) can be uniquely written as \(\sigma = \sigma_{1}\ldots\sigma_{k}\), where the \(\sigma_{j}\)’s are disjoint cycles of length \(c_j\), and we assume WLOG that \(c_1 \geq \ldots \geq c_k\) and that \(c_1 + ... + c_k = n\) – that is, we add length \(1\) cycles for each element fixed by \(\sigma\) if needed. The tuple \((c_1,...,c_k)\) is called the <em>partition</em> of \(n\) associated with \(\sigma\), and note that since conjugating a given cycle by \(\pi\) doesn’t alter its length, it follows that \(\sigma^{\pi}\) and \(\sigma\) have the same partition w.r.t. \(n\), and hence \(\sigma' \in \sigma^{S_n}\) if, and only if, \(\sigma\) and \(\sigma'\) have the same partition, i.e., iff they both have cycles of same length when ordered in a non-increasing manner.</p> <h2 id="the-alternating-group">The Alternating Group</h2> <p>The symmetric group has many subgroups in general, but a rather important one is the <em>alternating group</em> \(A_n\). From the Proposition 1, we have a well-defined notion of parity of a permutation: we say that a permutation is <em>odd</em> if it can be decomposed as an odd number of transpositions, and <em>even</em> if it can be decomposed as an even number of transpositions. Equivalently, a permutation \(\sigma\) is odd if \(\sigma(F) = -F\) – where \(F\) is the polynomial defined in the proof of Proposition 1 –, and even if \(\sigma(F) = F\). The set \(A_n\) is then simply the set of all even permutations, which can be immediately verified to form a subgroup of \(S_n\).</p> <p>When considering the action of \(S_n\) on the polynomial ring \(\mathbb{Q}[x_1,...,x_n]\), we can look at the orbit of the polynomial \(F\), which by the previous arguments contains only two elements: \(F\) and \(-F\). On the other hand, we note that the even permutations are precisely those that fix \(F\), i.e., we can look at \(A_n\) as the stabilizer of \(F\) w.r.t. this action. By the Orbit-Stabilizer Theorem we thus obtain:</p> \[|A_n| = \frac{|S_n|}{2} = \frac{n!}{2}\] <p>This means that the index of \(A_n\) as a subgroup of \(S_n\) – i.e., the size of the set \(\{A_n\sigma\mid\sigma \in S_n\}\) – is precisely two, and thus \(A_n\) is a normal subgroup of \(S_n\). Note that this also implies that \(A_n\) is a maximal subgroup of \(S_n\), since any group that properly contains it must have order between \(n!/2\) and \(n!\), and also be a divisor of \(n!\), hence it must be the entire \(S_n\) – this is actually true for any subgroup of a finite group with prime index.</p> <p>If \(\sigma = (i_1,...,i_k)\) is a cycle of length \(k\), we’ve observed before that \(\sigma\) can be decomposed as a product of \(k-1\) transpositions, hence \(\sigma \in A_n\) if, and only if, \(k\) is odd. Also, if \(n \geq 3\), then \(A_n\) is in fact generated by the cycles of length \(3\). Indeed, if \(\sigma \in A_n\), then \(\sigma\) can be written as a product of an even number of transpositions, and since the product of two transpositions is always a product of cycles of length \(3\), the claim follows.</p> <p>We end this post with a description of the relationship between the conjugacy classes of \(S_n\) and \(A_n\). We are considering the action of \(S_n\) on itself via conjugation, hence the centralizer of any element \(\sigma \in S_n\) is given by:</p> \[C_{S_n}(\sigma) = \{\pi \in S_n|\sigma^\pi = \sigma\}\] <p>Now if \(C\) is a conjugacy class in \(S_n\), and if \(C \cap A_n \neq \emptyset\), then \(C \subseteq A_n\) since \(A_n\) is normal. Thus the conjugacy classes of \(S_n\) are either entirely contained in \(A_n\) or disjoint from it. If \(C \subseteq A_n\), again since \(A_n\) is normal, then \(A_n\) acts on \(C\) via conjugation, and thus we can indeed partition \(C\) as a disjoint union of conjugacy classes in \(A_n\). This means that we can obtain the conjugacy classes of \(A_n\) by studying the conjugacy classes of \(S_n\), and the next result will give us a particularly useful way of doing this.</p> <p><strong>Proposition 3:</strong> Let \(C\) be a conjugacy class of \(S_n\) with corresponding partition \((r_1,...,r_m)\) such that \(C \subseteq A_n\). Then the following are equivalent:</p> <ol> <li>The class \(C\) is a union of two conjugacy classes of \(A_n\);</li> <li>For every element \(c \in C\), its centralizer \(C_{S_n}(c)\) w.r.t. \(S_n\) is contained in \(A_n\);</li> <li>The numbers \(r_1,...,r_m\) are all odd and distinct.</li> </ol> <p><strong>Proof:</strong></p> <p>(1 \(\Rightarrow\) 2) We’ll show this by contrapositive. Assume that \(c \in C\) is an element such that \(H = C_{S_n}(c) \not\subseteq A_n\), hence we obtain the following chain of subgroups:</p> \[A_n \lneq A_nH \leq S_n\] <p>and since \(A_n\) is maximal, it follows that \(A_nH = S_n\), thus</p> \[|S_n| = \frac{|A_n||H|}{|A_n\cap H|} = \frac{|A_n||H|}{|C_{A_n}(c)|}\] <p>since \(C_{A_n}(c) = H \cap A_n\), and from the previous equation we obtain:</p> \[|C_{A_n}(c)| = \frac{H}{2}\] <p>Now, if we let \(C'\) be the conjugacy class of w.r.t. \(A_n\), and using the Orbit-Stabilizer theorem, we conclude that:</p> \[|C||H| = 2|C'||C_{A_n}(c)|\] <p>hence \(\vert C\vert = \vert C'\vert\), implying that \(C\) is a conjugacy class of \(A_n\).</p> <p>(2 \(\Rightarrow\) 1) Noting that \(C_{A_n}(c) = C_{S_n}(c) \cap A_n\), the hypothesis implies that \(C_{A_n}(c) = C_{S_n}(c)\), hence by the Orbit-Stabilizer Theorem, any conjugacy class \(C_i\) of \(c\) in \(A_n\) will satisfy:</p> \[\frac{|A_n|}{|C_i|} = \frac{|S_n|}{|C|}\] <p>thus \(\vert C_i\vert = \vert C\vert/2\), and since the conjugacy classes are disjoint, this implies that \(C = C_1 \cup C_2\), where \(C_1,C_2\) are conjugacy classes of \(A_n\).</p> <p>(2 \(\Rightarrow\) 3) Let \(\sigma \in C\), and let</p> \[\sigma = \sigma_1\ldots\sigma_m\] <p>be its decomposition into disjoint cycles. If there exists some even \(r_i\), then the corresponding cycle \(\sigma_i\) does not belong to \(A_n\). On the other hand, note that</p> \[\sigma^{\sigma_i^{-1}} = \sigma\] <p>since the cycles are disjoint, hence \(\sigma_i^{-1} \in C_{S_n}(\sigma)\), but \(\sigma_i^{-1} \not\in A_n\), which contradicts item 2, thus all \(r_i\)’s must indeed be odd. Now if WLOG \(r_1 = r_2\), then let</p> \[\sigma_1 = (x_1,...,x_{r_1}),\sigma_2 = (y_1,...,y_{r_2})\] <p>and define</p> \[\sigma' = (x_1,y_1)(x_2,y_2)\ldots(x_{r_1},y_{r_2})\] <p>and since \(r_1\) is odd, it follows that \(\sigma' \not\in A_n\). On the other hand, we have</p> \[(\sigma_1\sigma_2)^{\sigma'} = \sigma_1\sigma_2\] <p>thus \(\sigma' \in C_{S_n}(\sigma)\), which again contradicts item 2, and so the \(r_i\)’s are all distinct.</p> <p>(3 \(\Rightarrow\) 2) Assume that all \(r_i\)’s are odd and distinct, and assume that \(\pi \in S_n\) is an element of the centralizer of some \(\sigma \in C\), i.e., \(\sigma^\pi = \sigma\). Let \(\sigma = \sigma_1\ldots\sigma_m\)</p> <p>be its decomposition into disjoint cycles. Since the cycles are all distinct, we have</p> \[\sigma^\pi = \sigma_1^\pi\ldots\sigma_m^\pi\] <p>hence \(\pi \in C_{S_n}(\sigma_i)\) for all \(i\). On the other hand, the centralizer of any cycle in \(S_n\) is generated by the powers of the cycle together with other disjoint cycles. Since \(\pi\) lies in the centralizer of each \(\sigma_i\), this means that it is of the form:</p> \[\pi = \sigma_1^{a_1}\ldots\sigma_m^{a_m}\] <p>for some integers \(a_1,...,a_m\), and hence \(\pi \in A_n\), implying that \(C_{S_n}(\sigma) \leq A_n\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>From this result, we know now the following about the conjugacy classes of \(S_n\) and \(A_n\):</p> <ul> <li>The conjugacy classes of \(S_n\) are uniquely determined by the integer partitions of \(n\), i.e., for every tuple \((r_1,...,r_k)\), where \(r_1 + ... + r_k = n\) are integers, and \(r_1 \geq ... \geq r_k\), we obtain a unique conjugacy class of \(S_n\), and these are all conjugacy classes.</li> <li>If we start with some conjugacy class \(C\) of \(S_n\), then either \(C \subseteq A_n\) or \(C \cap A_n = \emptyset\).</li> <li>If \(C \subseteq A_n\), one of two possible scenarios may happen: if the centralizer of any element of \(C\) w.r.t. \(S_n\) is contained in \(A_n\), then \(C\) is the disjoint union of two conjugacy classes in \(A_n\), otherwise \(C\) is itself a conjugacy class in \(A_n\).</li> </ul> <p>These facts will be quite useful in proving that \(A_n\) is simple when \(n \geq 5\), and this in turn will lead us to conclude that \(S_n\) is not solvable, again when \(n \geq 5\).</p>]]></content><author><name></name></author><category term="math"/><category term="group-theory"/><summary type="html"><![CDATA[basics about the symmetric group and the alternating group]]></summary></entry><entry><title type="html">solvable groups</title><link href="https://henriqueassumpcao.github.io/blog/2024/solvable_groups/" rel="alternate" type="text/html" title="solvable groups"/><published>2024-06-23T00:00:00+00:00</published><updated>2024-06-23T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/solvable_groups</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/solvable_groups/"><![CDATA[<p>A couple of years ago I remember watching a 3Blue1Brown video about the unsolvability of the quintic, and being extremely excited by it but also a bit anguished by the fact that I thought I probably wouldn’t ever truly understand it. At the time I was in high school and, although I liked math, I thought that pure math just wasn’t for me. Fortunately that perception changed quite a bit during my undergraduate journey, and now I just finished a course in Group Theory and Galois Theory that proved the unsolvability of the quintic at the very end of the course. I don’t claim to ‘‘truly’’ understand this result – whatever that means – but I do think I have a far better understanding of it than my past self would’ve ever dreamed of, and I’m quite happy about it.</p> <p>For these and other reasons, I decided to make a series of posts that will eventually lead up to the Fundamental Theorem of Galois Theory and the unsolvability of the quintic. I will start with some Group Theory basics – in this post namely solvable groups – and then go through the basics of Galois extensions and the whole bunch.</p> <h2 id="derived-and-characteristic-subgroups">Derived and Characteristic Subgroups</h2> <p>We start with a group \(G\), and we define the <em>commutator</em> of two elements \(a,b\) in \(G\) as</p> \[[a,b] := a^{-1}b^{-1}ab\] <p>hence \([a,b] = 1\) iff \(a\) and \(b\) commute. We can then define the <em>derived subgroup</em> \(G'\) of \(G\) as the subgroup generated by all commutators, that is,</p> \[G' := \langle [a,b]:a,b \in G\rangle\] <p>We say that a subgroup \(K \leq G\) is a <em>characteristic subgroup</em> if it is invariant under automorphisms of \(G\), in other words, if \(\varphi \in \text{Aut}(G)\), then \(\varphi(K) = K\). Since conjugation by any element of \(G\) is an automorphism, it follows that any characteristic subgroup is normal, and in particular, we can see that</p> \[\varphi([a,b]) = [\varphi(a),\varphi(b)]\] <p>hence \(G'\) is also a characteristic subgroup. We now prove some basic facts about these subgroups.</p> <p><strong>Proposition 1:</strong> If \(G\) is a group, \(K,H \leq G\) are subgroups where \(K \subseteq H\), then the following hold:</p> <ul> <li>If \(K\) is a characteristic subgroup of \(H\), and \(H\) is a characteristic subgroup of \(G\), then \(K\) is also a characteristic subgroup of \(G\);</li> <li>If \(K\) is a characteristic subgroup of \(H\), and \(H\) is a normal subgroup of \(G\), then \(K\) is also a normal subgroup of \(G\);</li> <li>The quotient \(G/G'\) is an abelian group, and if \(N \trianglelefteq G\) is a normal subgroup such that \(G/N\) is abelian, then \(G' \leq N\).</li> </ul> <p><strong>Proof:</strong></p> <ul> <li>Let \(\varphi \in \text{Aut}(G)\) be a group automorphism, and note that since \(H\) is characteristic w.r.t. \(G\), the restriction of \(\varphi\) onto \(H\) is also an automorphism of \(H\), and since \(K\) is contained in \(H\), this implies that \(K\) is invariant w.r.t. \(\varphi\), and thus it is also characteristic w.r.t. \(G\).</li> <li>Since any conjugation by an element in \(G\) is an automorphism of \(G\), the result follows by an argument similar to the previous item.</li> <li>Let \(aG',bG' \in G/G'\) be two elements of the quotient group, and note that \([aG',bG'] = [a,b]G' = 1_{G'}\), hence \(aG',bG'\) commute, implying that the quotient is indeed abelian. Now if \(G/N\) is abelian for some normal subgroup \(N\), then for any \(aN,bN \in G/N\), we have \([aN,bN] = 1\), thus \(N\) contains all elements of the form \([a,b]\), and hence it contains \(G'\), as desired.</li> </ul> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>The quotient \(G/G'\) is often called the <em>abelianization</em> of \(G\).</p> <h2 id="normal-subnormal-and-derived-series">Normal, subnormal and derived series</h2> <p>If \(G\) is a group, and if</p> \[G = G_0 \geq G_1 \geq \ldots \geq G_k = 1\] <p>is a finite chain of subgroups, we say call this chain a <em>normal series</em> if each \(G_i\) is a normal subgroup of \(G\). We say that a series is <em>subnormal</em> if for each \(i\) we have \(G_{i+1} \trianglelefteq G_i\), and of course any normal series is also by definition subnormal. We say that a group \(G\) is solvable if there exists a normal series of subgroups</p> \[G = G_0 \geq G_1 \geq \ldots \geq G_k = 1\] <p>such that each quotient \(G_{i}/G_{i+1}\) is abelian. Naturally, any abelian group is solvable, and it can also be shown that any \(p\)-group is also solvable.</p> <p>Solvable groups are intimately connected to Galois Theory, and we will ultimately see that any solvable group gives rise to a polynomial with roots that can be expressed with the usual arithmetic expressions – addition, subtraction, multiplication and division – and a finite number of root operations – that is, taking the \(n\)-th root for some natural \(n\). A key result in this connection is the fact that, for values of \(n\) greater than or equal to \(5\), the symmetric group on \(n\) elements \(S_n\) is not solvable. There a multiple ways of proving this fact, but in the next post we’ll see that this actually follows from the fact that the altenating group \(A_n\) is simple for \(n \geq 5\) – that is, contains no proper nontrivial normal subgroups.</p> <p>Given any group \(G\), we can always consider the following series:</p> \[G = G^{(0)} \geq G^{(1)} \geq \ldots \geq G^{(k)} \geq \ldots\] <p>where \(G^{(1)} = G'\), and \(G^{(i+1)}\) is the derived subgroup of \(G^{(i)}\). From the previous proposition, it follows that this series is normal: \(G^{(i)}\) is both characteristic and normal w.r.t. \(G\) by induction, hence \(G^{(i+1)}\) also is. It also follows that each quotient \(G^{(i)}/G^{(i+1)}\) is abelian, and thus we have a pretty natural way of checking if a given group is solvable: if we manage to find some \(k\) for which \(G^{(k)} = 1\), we are done. What we’ll see now is that this is in fact a necessary and sufficient condition.</p> <h2 id="characterization-and-properties-of-solvability">Characterization and properties of solvability</h2> <p>We can now characterize solvable groups in terms of the derived series.</p> <p><strong>Theorem 1:</strong> If \(G\) is a group, the following are equivalent:</p> <ol> <li>The group \(G\) is solvable;</li> <li>There exists a subnormal series of \(G\) in which each of the quotients is abelian;</li> <li>There exists some \(k\) for which \(G^{(k)} = 1\).</li> </ol> <p><strong>Proof:</strong></p> <p>(1 \(\Rightarrow\) 2) This simply follows from the fact that any normal series is also subnormal.</p> <p>(2 \(\Rightarrow\) 3) Let</p> \[G = G_0 \trianglerighteq G_1 \trianglerighteq \ldots \trianglerighteq G_k = 1\] <p>be a subnormal series in which each quotient \(G_i/G_{i+1}\) is abelian. We’ll show that \(G^{(k)} \leq G_k\) and conclude the result, and in order for us to do this we proceed via induction on \(k\). For the base step, we note that since \(G/G_1\) is abelian, it follows that \(G' \leq G_1\). Now for the general case, we know that \(G_{k-1}/G_k\) is abelian, hence \(G_{k-1}' \leq G_k\), but since by the inductive hypothesis we have that \(G^{(k-1)} \leq G_{k-1}\), this implies that \(G^{(k)} \leq G_k\). Since \(G_k = 1\), this shows that \(G^{(k)} = 1\), as desired.</p> <p>(3 \(\Rightarrow\) 1) The derived series is a normal series that satisfies the desired properties.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>I’ll end this post with some properties of solvable groups that will be useful for future posts.</p> <p><strong>Proposition 2:</strong> If \(G\) is a group, then the following hold:</p> <ul> <li>Any subgroup of a solvable group is solvable;</li> <li>If \(\varphi\) is a homomorphism between two groups \(G,H\), then \(\varphi(G^{(i)}) = \varphi(G)^{(i)}\) for any \(i\). In particular, solvability is a property that is preserved under isomorphisms;</li> <li>If \(G\) is a solvable group and \(N\) is a normal subgroup, then \(G/N\) is solvable;</li> <li>If \(N\) is a normal subgroup of \(G\) such that both \(N\) and \(G/N\) are solvable, then \(G\) is solvable.</li> </ul> <p><strong>Proof</strong>:</p> <ul> <li>Let \(G \geq G^{(1)} \geq \ldots \geq G^{(k)} = 1\) be the derived series of \(G\). We’ll show by induction on \(k\) that \(H^{(k)} \leq G^{(k)}\) and conclude the result. For the base case, since \(G/G'\) is abelian, it follows that \(HG'/G' \leq G/G'\) is also abelian, but since \(HG'/G'\) is isomorphic to \(H/H\cap G'\), it follows that \(H' \leq H\cap G' \leq G'\). Now for the general case, let \(G^{(k-1)}/G^{(k)}\) be abelian, and note then that \(H^{(k-1)}G^{(k)}/G^{(k)}\) must also be abelian, and again since the latter is isomorphic to \(H^{(k-1)}/H^{(k-1)}\cap G^{(k)}\), it follows that \(H^{(k)} \leq G^{(k)} = 1\), as desired.</li> <li>Note that if \(G,H\) are groups and \(\varphi\) is an homomorphism, then \(\varphi([a,b]) = [\varphi(a),\varphi(b)]\) for any \(a,b \in G\), hence \(\varphi(G') = \varphi(G)'\), which by induction immediately implies that \(\varphi(G^{(i)}) = \varphi(G)^{(i)}\) for any \(i\).</li> <li>Let \(G \geq G^{(1)} \geq \ldots \geq G^{(k)} = 1\) be the derived series of \(G\), and let \(\pi\) be the canonical projection of \(G\) onto \(G/N\). Note then that by the previous item we have \(\pi(G^{(i)}) = (G/N)^{(i)}\) since \(\pi\) is surjective, hence \(1 = \pi(G^{(k)}) = (G/N)^{(k)}\), implying that \(G/N\) is solvable.</li> <li>Let \(k,l\) be such that \((G/N)^{(k)} = 1,N^l = 1\), and consider the subgroup \(G^{(k+l)}\) of \(G\). We note that \(\pi(G^{(k)}) = (G/N)^{(k)} = 1\), hence \(G^{(k)} \leq N\), and thus \(G^{(k+l)} \leq N^{(l)} = 1\), implying that \(G^{(k+l)} = 1\), and so we conclude that \(G\) is solvable.</li> </ul> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\]]]></content><author><name></name></author><category term="math"/><category term="group-theory"/><summary type="html"><![CDATA[basics about solvable groups]]></summary></entry><entry><title type="html">sylow theorems</title><link href="https://henriqueassumpcao.github.io/blog/2024/sylow_theorems/" rel="alternate" type="text/html" title="sylow theorems"/><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/sylow_theorems</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/sylow_theorems/"><![CDATA[<p>This semester I’m taking a course in Group Theory and Galois Theory at UFMG. This post will be about the famous Sylow theorems, and I plan on making a follow-up post about the fundamental theorem of finite abelian groups.</p> <p>Let \(G\) be a finite group of order \(n\), and let \(\nu_p(n)\) denote the largest power of \(p\) that divides \(n\), where \(p\) is a prime. Any subgroup \(H \leq G\) of \(G\) with order \(\nu_p(n)\) is called a Sylow \(p\)-subgroup. We’ll show that such groups exists for all primes \(p\), and then proceed to prove some interesting properties related to them.</p> <h2 id="some-basic-facts-from-number-theory">Some basic facts from number theory</h2> <p>We’ll need two basic results from number theory in order to prove the Sylow Theorems.</p> <p><strong>Theorem (Legendre’s Formula):</strong> If \(n\) is a natural number and \(p\) is a prime, then</p> \[\nu_p(n!) = \sum_{k \geq 1}\Big\lfloor\frac{n}{p^k}\Big\rfloor\] <p><strong>Proof:</strong> The proof of this result becomes much easier when we get some intuitive feeling for what \(\nu_p(n!)\) represents. This quantity is precisely the exponent of \(p\) in the prime factorization of the input \(n!\), but since \(n!\) is a product of numbers and \(p\) is prime, this implies that if \(p\) divides \(n!\), then it must divide one of terms in the factorial, i.e., \(p\) must divide a number that is lesser than or equal to \(n\). This means that there is a multiple of \(p\) that is at most \(n\), and conversely any number \(m \leq n\) that is a multiple of \(p\) contributes with at least one factor of \(p\) in the prime factorization of \(n!\). Thus, the number of multiples of \(p\) is a lower bound on \(\nu_p(n!)\), since each contributes with at least one factor of \(p\). However, after factoring \(p\) from each of these, there might still be multiples of \(p\) left, namely those that are multiples of \(p^2\), and so we can again factor \(p\) out of each of these. We can repeat this for each \(p^k\) ultil \(p^k &gt; n\), i.e., until \(k &gt; \lfloor\log_p(n)\rfloor\), and at this point there will be no factors of \(p\) left in \(n!\).</p> <p>Now lets make this precise. Let \(n = qp + r\), where \(q,r\) are non-negative integers such that \(0 \leq r &lt; p\), and note that</p> \[\frac{n}{p} = q + \frac{r}{p}\] <p>and since \(r/p &lt; 1\), it follows that</p> \[\Big\lfloor\frac{n}{p}\Big\rfloor = q\] <p>This means that \(\lfloor\frac{n}{p}\rfloor\) is precisely the number of multiples of \(p\) that are lesser than or equal to \(n\), and also note that \(q &gt; 0\) iff \(p \leq n\). Now consider the following inductive procedure:</p> <ul> <li>Each multiple of \(p\) contributes with at least one factor of \(p\) in \(n!\), and there exactly \(\lfloor\frac{n}{p}\rfloor\) such multiples, so we factor \(p\) from each of them, that is, we factor out \(p^{\lfloor\frac{n}{p}\rfloor}\) from \(n!\)</li> <li>If we now look at \(n!/p^{\lfloor\frac{n}{p}\rfloor}\), any multiples of \(p\) will again contribute with at least one factor of \(p\), but any such multiples must also be multiples of \(p^2\), and there are precisely \(\lfloor\frac{n}{p^2}\rfloor\) of those, thus we can factor out \(p^{\lfloor\frac{n}{p^2}\rfloor}\)</li> <li>We can repeat this process until \(k &gt; \lfloor\log_p(n)\rfloor\), and at this point we’ll have factored out every possible factor of \(p\) from each multiple of \(p\) that is at most \(n\)</li> </ul> <p>After doing this, we’ll have factored out</p> \[\sum_{k=1}^{\lfloor\log_p(n)\rfloor}\Big\lfloor\frac{n}{p^k}\Big\rfloor = \sum_{k \geq 1}\Big\lfloor\frac{n}{p^k}\Big\rfloor\] <p>factors of \(p\) from \(n!\), showing that</p> \[\nu_p(n!) \geq \sum_{k \geq 1}\Big\lfloor\frac{n}{p^k}\Big\rfloor\] <p>On the other hand, by the arguments made before, any factor of \(p\) in \(n!\) must come from a multiple of \(p\) that is at most \(n\), but by construction the previous procedure factors out all possible factors of \(p\) from each of these multiples, hence the previous quantity is also an upper bound on \(\nu_p(n!)\), which proves the desired result.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>We can now use this fact to prove an important Lemma.</p> <p><strong>Lemma:</strong> If \(p\) is a prime and \(n\) is a non-negative integer, then \(p\) does not divide</p> \[{n \choose p^{\nu_p(n)}}\] <p><strong>Proof:</strong> Let \(\alpha = \nu_p(n)\), then</p> \[{n \choose p^{\alpha}} = \frac{n!}{(p^{\alpha})!(n-p^{\alpha})!}\] <p>The idea now is to show that \(p^{\nu_p(n!)}\) appears in this denominator. By Legendre’s formula, we have</p> \[\nu_p(p^{\alpha}!) = \sum_{k\geq 1}\Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor\quad\text{and}\quad \nu_p((n-p^{\alpha})!) = \sum_{k\geq 1}\Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor\] <p>Now note that if \(a,b\) are non-negative integers, then the exponent of \(p\) in the prime factorization of \(ab\) will be the sum of the exponents of \(p\) in the factorizations of \(a\) and \(b\), i.e., \(\nu_p(ab) = \nu_p(a) + \nu_p(b)\). This implies that</p> \[\nu_p((p^{\alpha})!(n-p^{\alpha})!) = \sum_{k\geq 1} \Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor + \Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor\\\] <p>Now fix \(k \geq 1\) and note that</p> \[n = \Big\lfloor\frac{n}{p^k}\Big\rfloor p^k + r\] <p>where \(0 \leq r &lt; p^k\) and \(\Big\lfloor\frac{n}{p^k}\Big\rfloor\) are unique, on the other hand</p> \[\begin{split} n &amp;= (n-p^{\alpha}) + p^{\alpha}\\ &amp;= (\Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor p^k + r') + (\Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor p^k + r'')\\ &amp;= (\Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor + \Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor)p^k + (r'+r'')\\ \Rightarrow \Big\lfloor\frac{n}{p^k}\Big\rfloor p^k + r &amp;= (\Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor + \Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor)p^k + (r'+r'') \end{split}\] <p>and since \(0 \leq r',r'' &lt; p^k\), the uniqueness of the euclidean division implies that \(r = r' + r''\) and</p> \[\Big\lfloor\frac{n}{p^k}\Big\rfloor = \Big\lfloor\frac{n-p^{\alpha}}{p^k}\Big\rfloor + \Big\lfloor\frac{p^{\alpha}}{p^k}\Big\rfloor\] <p>for all \(k \geq 1\), hence</p> \[\nu_p(n!) = \nu_p((p^{\alpha})!(n-p^{\alpha})!)\] <p>implying that the largest exponent of p in the factorization \(n!\) already appears in \((p^{\alpha})!(n-p^{\alpha})!\), and so \(p\) does not divide \({n \choose p^{\alpha}}\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="the-sylow-theorems">The Sylow theorems</h2> <p><strong>Theorem (First Sylow theorem):</strong> If \(G\) is a finite group of order \(n\), and \(p\) be a prime, then there exists a Sylow \(p\)-subgroup of \(G\).</p> <p><strong>Proof:</strong></p> <p>Let \(\nu_p(n) = \alpha\), and consider \(F = \{ X \subseteq G:\vert X\vert = p^\alpha \}\) as the set of all subsets of \(G\) with cardinality precisely \(p^\alpha\), and note that the cardinality of \(F\) is given by</p> \[{n \choose p^\alpha}.\] <p>We observe that \(G\) acts on \(F\) via right multiplication</p> \[g \in G,X \in F:Xg = \{xg:x \in X\}\] <p>The orbits form a partition of \(F\), and since by the previous Lemma we know that \(p\) does not divide \(\vert F\vert\), it follows that there must be some orbit \(GX\) s.t. \(p\) does not divide \(\vert GX\vert\). Let \(H = G_X\) denote the stabilizer of \(X\) w.r.t. \(G\), and this subgroup will be our candidate for a Sylow \(p\)-subgroup of \(G\). By the Orbit-Stabilizer theorem we have</p> \[\vert G\vert = \vert GX\vert \vert G_X\vert.\] <p>Since by definition \(p^\alpha\) divides \(\vert G\vert\) and since \(p\) does note divide \(\vert GX\vert\), it follows that \(p^\alpha\) divides \(\vert H\vert\), hence \(p^\alpha \leq \vert H\vert\). Now in order to prove the upper bound, we observe that \(H\) acts on \(X\) via right multiplication since \(H \leq G\), and if \(x \in X\), the the stabilizer of \(x\) w.r.t. to \(H\) is</p> \[H_x = \{g \in H:xg = x\} = \{1\},\] <p>since both \(x,g \in G\), hence again by the Orbit-Stabilizer theorem we have</p> \[\vert H\vert = \vert Hx\vert \vert H_x\vert = \vert Hx\vert\] <p>however note that \(X\) is partitioned by the orbits \(Hx\), hence \(\vert Hx\vert \leq \vert X\vert = p^\alpha\), thus</p> \[\vert H\vert = \vert Hx\vert \leq p^\alpha\] <p>implying that \(\vert H\vert = p^\alpha\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p><strong>Theorem (Second Sylow theorem):</strong> If \(G\) is a finite group of order \(n\), and \(p\) be a prime, then if \(P,Q\) are Sylow \(p\)-subgroups, then \(P\) and \(Q\) are conjugate, i.e., there exists some \(g \in G\) such that \(Q = P^g = \{gxg^{-1}\vert x \in P\}\).</p> <p><strong>Proof:</strong> Again let \(\nu_p(G) = \alpha\), and let \(P^G = \{P^g\vert g \in G\}\) be the orbit of \(P\) w.r.t. the action of \(G\) via conjugation on the set of all of its subgroups. \(G\) acts on \(P^G\) via conjugation, hence if we fix \(K = P^g\) for some \(g \in G\), we get via the Orbit-Stabilizer theorem we have</p> \[\vert G\vert = \vert K^G\vert \vert N_G(K)\vert\] <p>where \(N_G(K) = \{h \in G:(K)^h = K\}\) is the normalizer of \(K\) in \(G\), and clearly \(K \leq N_G(K)\). Since \(K\) is conjugate to \(P\), it follows that it is a Sylow \(p\)-group, and it also is a subgroup of its normalizer, thus by Lagrange’s theorem we conclude that \(p^\alpha\) divides \(\vert N_G(K)\vert\). Since this is the largest power of \(p\) that divides \(\vert G\vert\), it follows that \(p\) does not divide \(\vert K^G\vert\). On the other hand, we also know that \(Q\) acts on \(K^G\) via conjugation, so let \(C_1,...,C_m\) be the orbits of such action, hence</p> \[K^G = C_1 \sqcup C_2 \sqcup\ldots\sqcup C_m\] <p>Again by the Orbit-Stabilizer theorem, we know that \(\vert C_i\vert\) must divide \(\vert Q\vert = p^\alpha\), thus \(\vert C_i\vert = p^{\beta_I}\) for some \(\beta_i \geq 0\), hence</p> \[\vert K^G\vert = p^{\beta_1} + ... + p^{\beta_m}\] <p>but since \(p\) does not divide \(\vert K^G\vert\), this means that there is some \(i\) s.t. \(\beta_i = 0\), i.e., \(\vert C_i\vert = 1\), implying that \(C_i = \{K^{f}\}\) for some \(f \in G\) such that \((K^{f})^h = K^{f}\) for any \(h \in Q\), i.e., \(Q \leq N_G(K^{f})\). This means that \(Q\) normalizes \(K\), and thus \(KQ\) is a subgroup of \(G\) with order</p> \[\vert KQ\vert = \frac{\vert K\vert \vert Q\vert }{\vert K \cap Q\vert } = \frac{p^\alpha p^\alpha}{\vert K \cap Q\vert }\] <p>but since \(KQ \leq G\) and \(\vert KQ\vert\) divides \(G\), this means that \(\vert KQ\vert \leq p^\alpha\), implying that \(\vert K \cap Q\vert\) must be a multiple of \(p^\alpha\), but since both \(K\) and \(Q\) are Sylow \(p\)-groups, this implies that \(\vert K \cap Q\vert = p^\alpha = \vert K\vert = \vert Q\vert\), hence \(K=Q\). Since \(K = P^g\), this implies that \(Q\) and \(P\) are conjugate, which concludes the proof.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p><strong>Theorem (Third Sylow theorem):</strong> The number of Sylow \(p\)-groups of a finite group is congruent to \(1\) modulo \(p\).</p> <p><strong>Proof:</strong> Let \(P\) be a \(p\)-Sylow subgroup, thus by the previous theorem we know that \(P^G\) is the set of all \(p\)-Sylow subgroups, hence we need to show that \(\vert P^G\vert \equiv 1 \mod p\) . Note that \(P\) acts on \(P^G\) by conjugation, and that \(\{P\}\) is an orbit of such action, since \((P^1)^x = P\) for all \(x \in P\). Now assume that there is some \(\{R\}\) that is also an orbit of size \(1\) in this action, thus \(R^x = R\) for any \(x \in P\), implying that \(P \leq N_G(R)\), i.e., \(P\) normalizes \(R\). We’ve seen in the previous proof that this means that \(PR\) is a subgroup of \(G\), but since they are both \(p\)-Sylow subgroups, this implies that \(P = R\). Hence there is only one orbit of size one in the action of \(P\) on \(P^G\), and since the size of the orbits must divide the order of \(P\), this implies that all other orbits have size equal to some positive power of \(p\). Now since these same orbits partition \(P^G\), we get that \(\vert P^G\vert = 1 + pk\), where \(k \in \mathbb{N}\), hence \(\vert P^G\vert \equiv 1 \mod p\), as desired.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>There are some some results in mathematics that are really important and yet have quite boring and laborius proofs that don’t highligh some specially interesting idea, but this is certainly not the case with the Sylow Theorems. The results in and of themselves are extremely important – e.g. one of the many proofs of fundamental theorem of algebra requires the first theorem – however the techniques used to prove them are to me their most interesting aspect. The idea of looking at the action of a subgroup on the orbits of another is something that in retrospect seems like a rather straighforward idea, but is something that I doubt I would have come up with by myself, and I’m quite glad that I’ve added this technique to my “toolbox”.</p>]]></content><author><name></name></author><category term="math"/><category term="group-theory"/><summary type="html"><![CDATA[proving the famous theorems related to sylow subgroups]]></summary></entry><entry><title type="html">completion of a metric space</title><link href="https://henriqueassumpcao.github.io/blog/2024/metric_completion/" rel="alternate" type="text/html" title="completion of a metric space"/><published>2024-04-12T00:00:00+00:00</published><updated>2024-04-12T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/metric_completion</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/metric_completion/"><![CDATA[<p>I’m taking a course in Topology this semester, and I came across this interesting exercise about completions of metric spaces. We will show that any metric space can be extended to a complete metric space, i.e., a space where all Cauchy sequences converge.</p> <h2 id="some-basic-concepts">Some basic concepts</h2> <p>Let \((E,d)\) be a metric space, i.e., a set with a function \(d:E\times E\mapsto \mathbb{R}\) such that</p> <p>(i) \(d(x,y)= 0\) iff \(x =y\)</p> <p>(ii) \(d(x,y) = d(y,x)\) (Symmetry)</p> <p>(iii) \(d(x,y) \leq d(x,z) + d(z,x)\) (Triangle Inequality)</p> <p>We’ll denote a sequence of elements of \(E\) by \((x_n) \subset E\), and we recall that a sequence is Cauchy if for every \(\epsilon &gt; 0\), there exists some natural \(K\) such that</p> \[i,j &gt; K \Rightarrow d(x_i,x_j) &lt; \epsilon\] <p>It is immediate that any convergent sequence in \(E\) is Cauchy, however the converse is not true in general. The elements in Cauchy sequences are getting arbitrarily close to one another as \(n\) increases, and so in some sense these sequences “want” to converge to something, but the problem is that this something may not be an element of the metric space, e.g., the sequence \(((1+1/n)^n) \subset \mathbb{Q}\) is Cauchy, however we know that it converges to Euler’s constant \(e\), which is irrational. Our goal here is to show that we can build a larger space where every Cauchy sequence converges to a point in that space, and in the case of \(\mathbb{Q}\) this will yield one of the possible constructions for the real numbers.</p> <p>Now recall that if \((E_1,d_1),(E_2,d_2)\) are metric spaces, a function \(f:E_1\mapsto E_2\) is called an isometry if</p> \[\forall x,y \in E_1: d_1(x,y) = d_2(f(x),f(y))\] <p>Note that from this definition it follows that an isometry is always injective and continuous, more specifically it is Lipschitz continuous with constant 1. Sometimes isometries are also required to be bijective, but we will make no such requirement. The main point here is that an isometry is a distance-preserving injective map, and so it preserves the metric structure of the domain in its image. Our goal is not only to construct a complete space from \(E\), but do this in a way that guarantees that \(E\) is contained in this larger space via an isometry.</p> <h2 id="the-space-of-cauchy-sequences">The space of Cauchy sequences</h2> <p>I think that the hardest part of this exercise is probably thinking about a good candidate for the space to complete \(E\). We’ll consider the following set</p> \[C[E] = \{(x_n) \subset E|(x_n)\text{ is Cauchy}\}\] <p>We’ll equip this set with a metric that captures whether how close two sequences are getting to each other as \(n\) increases arbitrarily, more specifically, we consider</p> \[D((x_n),(y_n)) = \lim_{n\rightarrow \infty}d(x_n,y_n)\] <p>After seeing this construction, it is not hard to believe that something related to this set will be our completion, however thinking about this construction in the first place is something that I found to be quite challenging. We first note that \(D\) is almost a metric, since it follows by definition that \(D\) is symmetric and respects the triangle inequality. However, it is not true that if \(D((x_n),(y_n)) = 0\) then \((x_n),(y_n)\) are the same, since any pair of distinct sequences that “want” to converge to the same point will have distance zero. For example, we can consider \(C[\mathbb{Q}]\) and note that the sequences \((1/n)\) and \((-1/n)\) both converge to zero, and thus</p> \[\lim_{n\rightarrow \infty}|1/n-(-1/n)| = \lim_{n\rightarrow \infty}(2/n) = 0\] <p>however they are clearly distinct. Formally, \(D\) is a pseudo-metric, and we can naturally transform a space with a pseudo-metric into a metric space by using equivalence relations. In order for us to do that, we define the equivalence relation \(\sim\) on \(C[E]\) given by</p> \[(x_n) \sim (y_n) \Longleftrightarrow D((x_n),(y_n)) = 0\] <p>Thus, we can consider the space</p> \[\overline{E} := C[E]/\sim = \{[(x_n)]|(x_n) \in C[E]\}\] <p>where \([(x_n)]\) denotes the equivalence class of \((x_n)\) w.r.t. \(\sim\). Intuitively, this means that we’ve aggregated all Cauchy sequences that “want” to converge to the same point into a single set, and now we are treating this set as a point in itself. From now on, we’ll denote \((x_n) \in C[E]\) by \(\overline{x}\), and we note that the function</p> \[\overline{D}([\overline{x}],[\overline{y}]) := D(\overline{x},\overline{y}),\overline{x} \in [\overline{x}],\overline{y} \in [\overline{y}]\] <p>is well-defined (follows from the triangle inequality), i.e., we can evaluate the distance between \([\overline{x}]\) and \([\overline{y}]\) by evaluating the distance between any representative of the respective equivalence classes w.r.t. \(D\).</p> <h2 id="constructing-an-isometry">Constructing an Isometry</h2> <p>Now we’ll explicitly build an isometry between \(E\) and \(\overline{E}\). Consider the following map</p> \[\begin{split} \iota:E&amp;\mapsto\overline{E}\\ \iota(x) &amp;= [(x,x,...,x,...)] \end{split}\] <p>That is, we map each point \(x \in E\) to the equivalence class of the constant sequence with each element equal to \(x\). Such sequence is clearly Cauchy, and it converges to \(x\), so \(\iota(x)\) is the set of all sequences that converge to \(x\). Now, we note that</p> \[\begin{split} \overline{D}(\iota(x),\iota(y)) &amp;= D((x,x,...,x,...),(y,y,...,y,...))\\ &amp;= \lim_{n\rightarrow \infty}d(x,y) = d(x,y) \end{split}\] <p>hence \(\iota\) is indeed an isometry, i.e., it is injective, continuous, and preserves the distances in \(E\). This means that, in a very concrete and well-defined sense, \(E\) is contained in \(\overline{E}\), and so our construction so far makes some sense in the context of metric spaces. We now prove an additional important result.</p> <p><strong>Lemma:</strong> \(\iota(E)\) is a dense subset of \(\overline{E}\). In other words, every point on \(\overline{E}\) can be approximated by a sequence of points in \(\iota(E)\).</p> <p><strong>Proof:</strong> Let \([\overline{x}] \in \overline{E}\) be an element of \(\overline{E}\), and note that by definition \(\overline{x}\) is a Cauchy sequence in \(E\), hence for every \(N \in \mathbb{N}\), we can find some \(K_N \in \mathbb{N}\) such that</p> \[i,j \geq K_N \Rightarrow d(x_i,x_j) &lt; 1/2N\] <p>In particular, we have that \(d(x_i,x_{K_N}) &lt; 1/2N\) for every \(i &gt; K\), which yields</p> \[\lim_{n\rightarrow \infty} d(x_n,x_{K_N}) \leq 1/2N &lt; 1/N \Rightarrow \overline{D}([\overline{x}],\iota(x_{K_N})) &lt; 1/N\] <p>Thus, we consider the sequence \((\iota(x_{N_K}))_{K \in \mathbb{N}} \subset \overline{E}\). If we fix \(\epsilon &gt; 0\), we can always find some natural \(N\) such that \(1/N &lt; \epsilon\), and so</p> \[i &gt; K_N \Rightarrow \overline{D}([\overline{x}],\iota(x_{K_N})) &lt; 1/N &lt; \epsilon\] <p>which shows that \((\iota(x_{N_K}))\) converges to \([\overline{x}]\) in \(\overline{E}\), implying that \(\iota(E)\) is dense in \(\overline{E}\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>This means that, not only is \(E\) contained in \(\overline{E}\) in a way that preserves distances, but also that we can approximate any element of \(\overline{E}\) by some sequence of elements of \(E\).</p> <h2 id="the-main-result">The main result</h2> <p>We are now ready to show that \(\overline{E}\) is complete, that is, if we have a Cauchy sequence \(([\overline{x}^{(k)}])_{k \in \mathbb{N}}\) in \(\overline{E}\), then it must converge to some \([\overline{x}]\) in \(\overline{E}\). Concretely, this means that as \(k\) gets arbitrarily large, the point to which the Cauchy sequences in \([\overline{x}^{(k)}]\) “want” to converge to gets arbitrarily close to the point that \([\overline{x}]\) “wants” to converge to.</p> <p><strong>Theorem:</strong> If \((E,d)\) is a metric space, than the space \((\overline{E},\overline{D})\) is a complete metric space. Moreover, there is an isometry \(\iota\) between \(E\) and \(\overline{E}\) such that \(\iota(E)\) is dense in \(\overline{E}\).</p> <p><strong>Proof:</strong> The trickiest part of this proof is finding a candidate for the convergent point of the sequence \(([\overline{x}^{(k)}])\). First, we note that \(([\overline{x}^{(k)}])\) converges to \([\overline{x}]\) iff \((\overline{x}^{(k)})\) converges to \(\overline{x}\) w.r.t. \(D\), for some elements \(\overline{x}^{(k)},\overline{x}\) of the respective equivalence classes, thus we only need to construct a Cauchy sequence \(\overline{x}\) in \(E\) such that for any \(\epsilon &gt; 0\) there exists some \(k \in \mathbb{N}\) so as</p> \[i &gt; k \Rightarrow \lim_{n\rightarrow \infty} d(x^{(i)}_n,x_n) &lt; \epsilon\] <p>where \(\overline{x}^{(i)} = (x_n^{(i)})\). In order to do this, we first use that each \(\overline{x}^{(k)}\) is itself a Cauchy sequence, i.e., for every \(k \in \mathbb{N}\), we can find some \(n_k \in \mathbb{N}\) such that</p> \[i,j \geq n_k \Rightarrow d(x^{(k)}_i,x^{(k)}_j) &lt; 1/k\] <p>hence our candidate will be the sequence</p> \[\overline{x} = (x^{(1)}_{n_1},x^{(2)}_{n_2},...,x^{(k)}_{n_k},...) = (x^{(k)}_{n_k})_{k \in \mathbb{N}}\] <p>We now need to show that this sequence is Cauchy, and also that \((\overline{x}^{(k)})\) converges to it. First, let \(\delta &gt; 0\), so we can find some \(k_1 \in \mathbb{N}\) such that \(1/k_1 &lt; \delta/6\), and we can also find some \(k_2 \in \mathbb{N}\) such that</p> \[i,j &gt; k_2 \Rightarrow D(\overline{x}^{(i)},\overline{x}^{(j)}) = \lim_{n\rightarrow \infty} d(x^{(i)}_n,x^{(j)}_n) &lt; \delta/6\] <p>since \((\overline{x}^{(k)})\) is also a Cauchy sequence. Now let \(k =\max(k_1,k_2)\), thus if \(i,j &gt; k\) we have</p> \[d(x^{(i)}_{n_i},x^{(j)}_{n_j}) \leq d(x^{(i)}_{n_i},x^{(i)}_{n}) + d(x^{(i)}_{n},x^{(j)}_{n}) + d(x^{(j)}_{n},x^{(j)}_{n_j})\] <p>and taking \(n &gt; \max(n_i,n_j)\), we obtain</p> \[\begin{split} d(x^{(i)}_{n_i},x^{(i)}_{n}) &amp;&lt; 1/i &lt; 1/k &lt; \delta/6\\ d(x^{(j)}_{n_j},x^{(j)}_{n}) &amp;&lt; 1/j &lt; 1/k &lt; \delta/6\\ \end{split}\] <p>Hence if \(n &gt; \max(n_i,n_j)\), this implies that</p> \[\begin{split} d(x^{(i)}_{n_i},x^{(j)}_{n_j}) &amp;&lt; \delta/3 + d(x^{(i)}_{n},x^{(j)}_{n})\\ \Rightarrow d(x^{(i)}_{n_i},x^{(j)}_{n_j}) &amp;= \lim_{n\rightarrow \infty} d(x^{(i)}_{n_i},x^{(j)}_{n_j}) \leq \delta/3 + \lim_{n\rightarrow \infty}d(x^{(i)}_{n},x^{(j)}_{n})\\ &amp;\leq \delta/3 + \delta/6 &lt; \delta \end{split}\] <p>and since \(\delta\) was arbitrary, this shows that \(\overline{x}\) is Cauchy.</p> <p>Now fix \(\epsilon &gt; 0\), and let \(k_1 \in \mathbb{N}\) be such that \(1/k_1 &lt; \epsilon/4\), thus</p> \[n &gt; n_{k_1} \Rightarrow d(x^{(k_1)}_n,x^{(k_1)}_{n_{k_1}}) &lt; 1/k_1 &lt; \epsilon/4\] <p>Since \(\overline{x}\) is itself Cauchy, there exists some \(k_2 \in \mathbb{N}\) such that</p> \[i &gt; k_2 \Rightarrow d(x^{(i)}_{n_i},x^{(k_2)}_{n_{k_2}}) &lt; \epsilon/4\] <p>Let \(k = \max(k_1,k_2)\), thus we obtain</p> \[\begin{split} n &gt; n_k &amp;\Rightarrow d(x^{(k)}_n,x^{(k)}_{n_k}) &lt; \epsilon/4\\ i &gt; k &amp;\Rightarrow d(x^{(i)}_{n_i},x^{(k)}_{n_k}) &lt; \epsilon/4 \end{split}\] <p>Combining these two inequalities, if \(j &gt; \max(k,n_k)\), we get that</p> \[\begin{split} d(x^{(k)}_j,x^{(j)}_{n_j}) &amp;\leq d(x^{(k)}_j,x^{(k)}_{n_k}) + d(x^{(j)}_{n_j},x^{(k)}_{n_k}) &lt; \epsilon/2\\ \Rightarrow D(\overline{x}^{(k)},\overline{x}) &amp;= \lim_{j \rightarrow \infty} d(x^{(k)}_j,x^{(j)}_{n_j}) \leq \epsilon/2 &lt; \epsilon \end{split}\] <p>and thus we can conclude that \(([\overline{x}^{(k)}])\) converges to \([\overline{x}]\), i.e., \(\overline{E}\) is complete.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>This result shows that we can indeed build a new space from \(E\) that is a complete metric space that “contains” \(E\) in a meaningful way, i.e., there is a dense subset of this new space that is isometric to \(E\). It is also easy to check that \(\iota(E) = \overline{E}\) iff \(E\) is complete, and through some extra work it can also be shown that \(\overline{E}\) is unique up to isometry, i.e., any other space \(\overline{E}'\) that contains a dense subset that is isometric to \(E\) is itself isometric to \(\overline{E}\).</p> <p>This gives an interesting way of thinking about the real numbers: an irrational number such as \(\pi\) can be viewed as the set of sequences of rational numbers that are getting arbitrarily close together and converging to something that isn’t rational, namely \(\pi\) itself. From the “perspective” of the rational numbers, the real numbers are nothing more than bundles of sequences that are getting really close together and “wanting” to converge to something. Sometimes this something turns out to be a rational number, but sometimes it turns out not to be, and when this is the case we simply identify this “thing” that isn’t rational with the set of sequences that are getting arbitrarily close to it.</p>]]></content><author><name></name></author><category term="math"/><category term="topology"/><summary type="html"><![CDATA[how to complete a metric space]]></summary></entry><entry><title type="html">the schur basis and some other decompositions</title><link href="https://henriqueassumpcao.github.io/blog/2024/matrix_decomp/" rel="alternate" type="text/html" title="the schur basis and some other decompositions"/><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/matrix_decomp</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/matrix_decomp/"><![CDATA[<p>A few months ago I was studying Bhatia’s Matrix Analysis book while I stumbled upon a pretty basic result that, for some reason, I had never proven before in any of my linear algebra classes: that every matrix over the complex numbers has an orthogonal basis such that it is in an upper triangular form (called the Schur basis). In this post I’ll prove this result, and then use it to prove that an arbitrary family of commuting matrices has a commom Schur basis, and also talk a bit about other interesting related results.</p> <h2 id="triangular-and-normal-matrices">Triangular and Normal matrices</h2> <p>Let \(V\) be a \(n\)-dimensional vector space over the complex numbers, and assume we have a matrix \(T\) in upper-triangular form, where we denote the \(ij\)-th element of \(T\) by \(T_{ij}\). The determinant of a triangular matrix is just the product of its diagonal elements, since any permutation other than the identity results in at least one zero appearing in the product of elements given by the determinant formula. Now note that</p> \[\varphi(T,\lambda) = \det(T - \lambda I)\] <p>where \(\varphi\) denotes the characteristic polynomial of \(T\), and since \(T - \lambda I\) is also upper triangular, we get that</p> \[\varphi(T,\lambda) = (T_{11} - \lambda)\cdot...\cdot(T_{nn} - \lambda)\] <p>and hence all of the eigenvalues of \(T\) are in its diagonal entries.</p> <p>We now prove that for any linear operator \(A\) on \(V\) there exists an orthogonal basis in which the matrix of such function is upper triangular. Note that this is equivalent to saying that there is an orthogonal basis \(\{v_1,...,v_n\}\) of \(V\) such that \(Av_i \in \text{span}_{\mathbb{C}}(v_1,...,v_i)\), and we can prove this via a simple induction on the dimension \(n\) of \(V\). If \(n = 1\) then any normalized non-zero vector satisfies the desired property. For an arbitrary \(n\), note that since \(\mathbb{C}\) is algebraically closed we can always find an eigenvalue \(\lambda\) with a non-zero eigenvector \(v_1\), and thus decompose the space as</p> \[V = \mathbb{C}v_1 \oplus (\mathbb{C}v_1)^\perp\] <p>where \(W = (\mathbb{C}v_1)^\perp\) is the \((n-1)\)-dimensional orthogonal complement of the span of \(v_1\). If we consider the orthogonal projection \(P\) onto \(W\), the function \(PA\) is a linear operator on \(W\), and thus by the inductive hypothesis we can find a basis \(\{v_2,...,v_n\}\) for \(W\) such that \((PA)(v_i) \in \text{span}_{\mathbb{C}}(v_2,...,v_i)\). Now if we consider \(\{v_1,...,v_n\}\), it is an orthogonal basis for \(V\), and since we can decompose the identity \(I = P' + P\), where \(P'\) is the projection onto the span of \(v_1\), we have that for every \(v_k\)</p> \[Av_k = ((P'+P)A)(v_k) = (P'A)(v_k) + (PA)(v_k)\] <p>By definition, \((P'A)(v_k)\) is in the span of \(v_1\), and by the induction hypothesis \((PA)(v_k)\) is in the span of \(v_2,...,v_k\), hence \(Av_k\) belongs to the span of \(v_1,...,v_k\), as desired. This basis is called the <em>Schur basis</em> of an operator, and from this it follows that for any matrix \(A\) there is an orthogonal matrix \(Q\) such that \(Q^*AQ\) is upper triangular.</p> <p>Recall that a linear operator \(A\) is called <em>normal</em> if it commutes with its adjoint, and in our case for any fixed basis the matrix of the adjoint is the conjugate transpose of the operator’s matrix. The spectral theorem states that an operator on a finite dimensional vector space over \(\mathbb{C}\) can be orthogonally diagonalized, i.e., written as \(Q\Lambda Q^*\) with \(Q\) orthogonal matrix, and \(\Lambda\) diagonal matrix of eigenvalues, if and only if, it is normal. This theorem can actually be regarded as a corollary of the existence of the Schur Basis. If we write \(A\) w.r.t. its Schur basis, we obtain</p> \[\begin{split} A &amp;= \begin{bmatrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n}\\ 0 &amp; a_{22} &amp; ... &amp; a_{2n}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 0 &amp; 0 &amp; ... &amp; a_{nn}\\ \end{bmatrix}\\ A^* &amp;= \begin{bmatrix} \overline{a_{11}} &amp; 0 &amp; ... &amp; 0\\ \overline{a_{12}} &amp; \overline{a_{22}} &amp; ... &amp; 0\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ \overline{a_{1n}} &amp; \overline{a_{2n}} &amp; ... &amp; \overline{a_{nn}}\\ \end{bmatrix} \end{split}\] <p>and since the matrices commute, if we look at the result of their diagonal products, it follows that any off-diagonal entry must be zero, and thus \(A\) and \(A^*\) must be diagonal matrices, and hence \(A\) can be orthogonally diagonalized.</p> <h2 id="nilpotent-matrices">Nilpotent matrices</h2> <p>An operator \(A\) on \(V\) is called <em>nilpotent</em> if there exists a non-negative integer \(r\) such that \(A^r = 0\), i.e., A composed with itself \(r\) times is equal to the zero map. From this definition it follows that any eigenvalue of \(A\) must be zero, and since we’ve observed that any operator on \(V\) has a Schur basis, it follows that there is a basis in which \(A\) is strictly upper triangular, i.e., all of its diagonal elements are zero. Conversely, any strictly upper triangular \(n \times n\) matrix satisfies \(N^n = 0\), and thus we get that an operator over a finite-dimensional vector space is nilpotent iff there is a basis in which its matrix is strictly upper triangular. From these facts and the Schur decomposition, it follows that for any matrix \(A\), there is an orthogonal matrix \(Q\) such that</p> \[Q^*AQ = D + N\] <p>where \(D\) is a diagonal matrix containing the eigenvalues of \(A\), and \(N\) is a nilpotent matrix in strict upper triangular form. This decomposition, however, is not unique, since the Schur basis is also not unique. We can, however, prove the uniqueness of the operators associated with \(D\) and \(N\), that is, we can prove that any operator \(A\) can be uniquely written as</p> \[A = D + N\] <p>where \(D\) is a <em>diagonalizable</em> operator (not necessarily in diagonal form), and \(N\) is a operator (again not necessarily in upper triangular form). Moreover, we can show that \(D\) and \(N\) are polynomials in \(A\), and hence they commute.</p> <p>For this, since we are assuming that \(V\) is a vector space over the complex numbers, we may assume that the minimal polynomial \(m_A\) of \(A\) is of the form</p> \[m_A(t) = (t-\lambda_1)^{r_1}\cdot\ldots\cdot(t-\lambda_k)^{r_k}\] <p>where each \(\lambda_i\) is an eigenvalue, and \(\lambda_i \neq \lambda_j\) if \(i \neq j\). From the spectral theorem, it follows that we can decompose \(V\) as</p> \[V = W_1 \oplus\ldots\oplus W_k\] <p>where \(W_i = \ker((T - \lambda_i I)^{r_i})\) is the <em>generalized eigenspace</em> associated with \(\lambda_i\), and we can consider the projections \(E_i\) in these spaces such that \(I = E_1 + ... + E_k\), and \(E_iE_j = \delta_{ij}E_i\). We can thus define the operator</p> \[D = \lambda_1 E_1 + ... + \lambda_k E_k\] <p>and we claim that \(D\) is diagonalizable. Indeed, this follows by simply observing that for any \(v \in V\), \(DE_iv = \lambda v\), hence \(W_i \subset \ker(D - \lambda I)\), and thus any basis of \(V\) given by the union of bases for each \(W_i\) is a basis of eigenvectors of \(D\). Now consider the operator</p> \[N = A - D\] <p>and note that \(N\) is a polynomial in \(A\), and so is \(D\), and thus they commute. Now since \(A = AE_1 + ... + E_k\), we have that</p> \[N = \sum_{i=1}^k (A-\lambda_i I)E_i\] <p>however, we recall that each \(E_i\) is a polynomial in \(A\), and hence commutes with \(A\), thus we have</p> \[\begin{split} N^2 &amp;= \sum_{i,j} (A-\lambda_i I)E_i(A-\lambda_j I)E_j \\ &amp;= \sum_{i,j} (A-\lambda_i I)(A-\lambda_j I)E_iE_j = \sum_i (A-\lambda_i I)^2E_i \end{split}\] <p>and in general we have that \(N^r = \sum_i (A-\lambda_i I)^rE_i\), then if \(r \geq \max(r_1,...,r_k)\), by definition of the minimal polynomial, we have that \(N^r = 0\), thus \(N\) is nilpotent. This shows that we can decompose any operator as a sum of a diagonalizable operator and a nilpotent operator. One way of thinking about this argument is that we are removing the diagonalizable part of \(A\) and then showing that this new operator is nilpotent, which makes sense since any non-zero nilpotent operator is not diagonalizable (since the only possible eigenvalue is zero).</p> <p>Now, we need to check that \(N\) and \(D\) are indeed unique (as operators, not as matrices). Assume that \(A = N + D = N' + D'\), and so</p> \[D - D' = N' - N\] <p>Since both \(N'\) and \(N\) are nilpotent, we can choose a sufficiently large \(r\) such that</p> \[(N' - N)^r = \sum_{i=0}^r (-1)^i {r \choose i}(N')^iN^{r-i} = 0\] <p>and so \(N' - N\) is also nilpotent. Note, however, that since both \(D\) and \(D'\) are diagonalizable and commute, they are simultaneously diagonalizable, and hence \(D-D'\) is also a diagonalizable operator, and so we have that a nilpotent operator is equal to a diagonalizable operator, however we know that the Schur basis of a nilpotent operator is a strictly upper triangular matrix, and that the schur basis of a diagonalizable operator is an eigenbasis, hence \(D-D' = N'-N = 0\), and we conclude that the operators are unique.</p> <h2 id="invariant-subspaces-and-commuting-matrices">Invariant subspaces and commuting matrices</h2> <p>If \(V\) is a vector space over \(\mathbb{C}\) of dimension \(n\), and \(A\) is a linear operator on \(V\), we can show that any \(A\)-invariant subspace of \(V\) contains an eigenvector of \(A\). Indeed, let \(W = \text{span}_{\mathbb{C}}(v_1,...,v_k)\) be a \(k\)-dimensional subspace of \(V\) such that \(AW \subset W\), let \(B = [v_1 \quad ... \quad v_k]\) be the \(n \times k\) matrix containing the basis of \(W\) in its columns. Since \(W\) is \(A\)-invariant, it follows that there must exist a \(k \times k\) matrix \(C\) such that</p> \[AB = BC\] <p>since \(AB = A[v_1\quad ... \quad v_k] = [Av_1 \quad ... \quad Av_k]\), and each \(Av_i\) can be expressed as a linear combination of \(v_1,...,v_k\). Since \(\mathbb{C}\) is algebraically closed (this argument works for any algebraically closed field), it follows that the matrix \(C\) itself has an eigenvector \(x\) with some eigenvalue \(\lambda\), hence</p> \[ABx = BCx = \lambda Bx\] <p>and thus \(Bx\) is an eigenvector of \(A\) (note that \(Bx\) must be non-zero since the vectors in \(B\) are linearly independent).</p> <p>This is quite useful because, now, if we consider two commuting linear operators \(A\) and \(B\), we can show that they have a common eigenvector, and, moreover, we can show that they have a common Schur basis. Let \(v\) be an eigenvector of \(A\) with eigenvalue \(\lambda\), and note that for any non-negative integer \(k\), we have that</p> \[B^kAv = \lambda B^kv \Rightarrow A(B^kv) = \lambda (B^kv)\] <p>hence \(B^kv\) is an eigenvector of \(A\) for every \(k \geq 0\). We can thus consider the space</p> \[W = \text{span}(v,Bv,B^2v,...)\] <p>and note that \(W\) is \(B\)-invariant, so there is some \(k\) for which \(B^kv\) is an eigenvector of \(B\), and thus a common eigenvector for \(A\) and \(B\) (not necessarily with the same eigenvalue). This can serve as a base case for showing that the result is true for any finite set of commuting matrices. Indeed, if we consider \(A_1,...,A_n\) as a set of commuting matrices, we can assume that there exists a common eigenvector \(v\) for \(A_1,...,A_{n-1}\), and using the same argument as before, we see that for any non-negative \(k\) that \(A_n^kv\) is a common eigenvector for all \(A_i\) with \(i &lt; n\). Again, we can consider the subspace</p> \[W = \text{span}(v,A_n^kv,A_n^{2k}v,...)\] <p>and note that it is \(A_n\)-invariant, and thus it also has an eigenvector of \(A_n\), which by definition must be an eigenvector for all \(A_1,...,A_{n-1}\), hence we have a common eigenvector of \(A_1,...,A_n\).</p> <p>Now, if \(A_1,...,A_n\) are a set of commuting matrices, we can show that they have a common Schur basis. Again, we proceed via induction on the dimension \(n\) of the space, and recall that we wish to show that there is a basis \(\{v_1,...,v_n\}\) of \(V\) such that</p> \[\forall i,j \in \{1,...,n\}:A_iv_j \in \text{span}_\mathbb{C}(v_1,...,v_j)\] <p>The base case of \(n=1\) is immediate. For the general case, we start with a common eigenvector \(v_1\) of \(A_1,...,A_n\) with respective eigenvalue \(\lambda_i\) for each \(A_i\), and decompose the space as</p> \[V = \mathbb{C}v_1 \oplus (\mathbb{C}v_1)^\perp\] <p>Note that if we consider a basis for \(V\) composed by a union of a basis for \(\mathbb{C}v_1\) and \((\mathbb{C}v_1)^\perp\), the matrices \(A_i\) will have the form</p> \[A_i = \begin{bmatrix} \lambda_i &amp; a_i\\ 0 &amp; A_{i}'\\ \end{bmatrix}\] <p>where \(a_i\) is a \(n-1\) dimensional line vector, \(0\) is the \(n-1\) dimensional zero vector, and \(A_i'\) is a \(n-1\times n-1\) block of \(A\). Now for two matrices \(A_i,A_j\), we have that</p> \[\begin{split} A_iA_j &amp;= \begin{bmatrix} \lambda_i\lambda_j &amp; c_{ij}\\ 0 &amp; A_{i}'A_{j}'\\ \end{bmatrix}\\ A_jA_i &amp;= \begin{bmatrix} \lambda_j\lambda_i &amp; d_{ji}\\ 0 &amp; A_{j}'A_{i}'\\ \end{bmatrix}\\ \end{split}\] <p>where \(c_{ij},d_{ji}\) are \(n-1\) dimensional line vectors (we can compute these explicitly, but we don’t really need that here). Since \(A_i,A_j\) commute, it follows that so do \(A_{i}',A_{j}'\), and thus \(A_1',...,A_n'\) are a finite set of commuting matrices operators on a \((n-1)\)-dimensional subspace of \(V\) (namely \((\mathbb{C}v_1)^\perp\)), and thus we obtain an orthogonal basis \(\{v_1,...,v_n\}\) for \(V\), and the same argument used to prove the existence of a Schur basis for a single matrix yields the desired result.</p> <h2 id="final-remarks">Final remarks</h2> <p>We’ve seen that any finite commuting set of matrices has a common Schur basis over the complex numbers, however this same result also implies that this set doesn’t need to be finite at all. This follows from the fact that, if \(\{A_i\}_{i \in \mathcal{I}}\) is an arbitrary family of commuting matrix with entries on \(\mathbb{C}\), then we can consider the <em>algebra</em> generated by such set. Recall that an algebra is just a ring which is also a module over some other ring, and in our case, an algebra is just a vector space that is closed w.r.t. matrix multiplication. Thus, the algebra generated by some set of matrices is nothing more than the set of all finite linear combinations of finite products between such matrices. For example, the algebra generated by some matrix \(A\) is just the set of all finite linear combinations of powers of \(A\). We say that an algebra is finite dimensional if it is so as a vector space, and hence the set of all \(n \times n\) matrices over \(\mathbb{C}\) is a finite dimensional algebra with dimension \(n^2\) (just take the matrices \(E_{ij}\) with only the \(ij\)-th entry non-zero and equal to one). Thus, any subalgebra of the full matrix algebra is also finite dimensional, and so the algebra generated by \(\{A_i\}_{i \in \mathcal{I}}\) must have a finite basis of matrices, and since the generating set is commutative, the entire algebra also is. Hence, we can find a common Schur basis for the matrices that form a basis of such algebra, and then any matrix in this algebra will also be in an upper triangular form. It also follows as a corollary that any family \(\{A_i\}_{i \in \mathcal{I}}\) of commuting normal matrices has a common orthogonal eigenbasis, i.e., there exists an orthogonal matrix \(Q\) such that \(Q^*A_iQ\) is diagonal for each \(i\).</p> <p>I’m currently studying matrix algebras and the Wedderburn-Artin theorems for my undergraduate thesis, so I intend on making more posts on connections between results in linear algebra and the study of matrix algebras and ring theory.</p>]]></content><author><name></name></author><category term="math"/><category term="linear-algebra"/><summary type="html"><![CDATA[a few cool results from linear algebra that I realized I had never proven before]]></summary></entry><entry><title type="html">free modules over principal ideal domains</title><link href="https://henriqueassumpcao.github.io/blog/2023/free_modules/" rel="alternate" type="text/html" title="free modules over principal ideal domains"/><published>2023-11-25T00:00:00+00:00</published><updated>2023-11-25T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2023/free_modules</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2023/free_modules/"><![CDATA[<p>We are interested in showing that free modules over principal ideal domains (PIDs) with finite basis behave in a similar fashion to vector spaces over fields: we wish to show that every submodule will also be free with a smaller basis. For this, we first tackle the notion of exact sequences of modules. Throughout this proof, \(R\) will denote a commutative ring with unity, and \(D\) will denote a PID. Consider the following diagram:</p> \[0 \mapsto M_1 \mapsto^f M_2 \mapsto^g M_3 \mapsto 0\] <p>where \(M_1,M_2,M_3\) are \(R\)-modules, and \(f,g\) are \(R\)-homomorphisms. We call this sequence exact if \(\ker(g)=\text{Im}(f)\). Given an exact sequence, we say that it is split exact if there exists some \(N\leq M_2: M_2 = \ker(g)\oplus N\), which also implies that \(M_2 = \text{Im}(f)\oplus N\). We will first prove two results related to split exact sequences, and then proceed to prove our main result.</p> <h2 id="a-characterization-of-split-exact-sequences">A characterization of split exact sequences</h2> <p><strong>Lemma 1:</strong> Given an exact sequence of the form given in the previous diagram, TFAE:</p> <ul> <li>The sequence is split exact.</li> <li>There exists \(\psi \in \text{Hom}_{R}(M_2,M_1)\) such that \(\psi \circ f = \text{id}_{M1}\).</li> <li>There exists \(\phi \in \text{Hom}_{R}(M_3,M_2)\) such that \(g \circ \phi = \text{id}_{M3}\).</li> </ul> <p>Under these conditions, we have that \(M_2 \cong M_1 \oplus M_3\).</p> <p><strong>Proof:</strong></p> <p><strong>(i)</strong>\(\Rightarrow\)<strong>(ii)</strong> Note that by the definition of our sequence, \(f\) is injective and \(g\) is surjective. Assume that \(M_2 = \text{Im}(f)\oplus N\), thus given any \(m_2 \in M_2\), we can write it uniquely as \(m_2 = f(m_1) + y\), where \(m_1 \in M_1,y \in N\), and note that \(m_1\) must be unique since \(f\) is injective. Thus, define \(\psi\) such that \(\psi(m_2) = \psi(f(m_1)+y)=m_1\). This function is well defined due to the above remarks, and note that the restriction of \(\psi\) onto \(\text{Im}(f)\) is a bijection, and also that \(\psi\) is an \(R\)-homomorphism: given \(m_2,m_2' \in M_2,\alpha \in R:\)</p> \[m_2+m_2' = f(m_1)+y+f(m_1')+y' = f(m_1+m_1') + y+y'\] \[\Rightarrow \psi(m_2+m_2')=\psi(m_2)+\psi(m_2')\] <p>and also that \(\psi(\alpha m_2) = \alpha \psi(m_2)\). Given any \(m_1 \in M_1\), note that \(\psi(f(m_1)) = m_1\) by definition, thus \(\psi \circ f = \text{id}_{M_1}\).</p> <p><strong>(ii)</strong>\(\Rightarrow\)<strong>(i)</strong> Now assume the existence of a function \(\psi\) as described in (ii). Let \(h:M_2 \mapsto M_2, h = f \circ \psi\). It is clearly an \(R\)-endomorphism on \(M_2\), and note that \(\text{Im}(h) \subset \text{Im}(f)\) by definition. On the other hand, given any \(m_1 \in M_1\), we have that \(f(m_1) = f(\psi(f(m_1))) = h(f(m_1)) \in \text{Im}(h)\), thus \(\text{Im}(h) = \text{Im}(f)\). Also, note that \(h\) is idempotent: \(f \circ \psi \circ f \circ \psi = f\circ \psi\), and so we can write \(M_2 = \text{Im}(h)\oplus\ker(h)=\text{Im}(f)\oplus\ker(n)\), and so the sequence is split exact.</p> <p><strong>(i)</strong>\(\Rightarrow\)<strong>(iii)</strong> Assume that \(M_2 = \ker(g)\oplus N\), and note that since \(g\) is surjective, given any \(m_3 \in M_3\), we can write it as \(m_3 = g(m_2)\), where \(m_2 \in M_2\). On the other hand, we can write each \(m_2\) uniquely as \(m_2 = x + y\), where \(x \in \ker(g),y \in N\), thus \(m_3 = g(y)\). We claim that this \(y\) is unique. Indeed, if there were another \(y' \in N\) s.t. \(m_3 = g(y')\), this would imply that \(g(y-y') = 0\), and so \(y-y' \in \ker(g)\cap N \Rightarrow y = y'\), since \(M_2\) is a direct sum. Thus, define \(\phi\) as \(\phi(m_3)=\phi(g(m_2))=\phi(g(y))=y\). This function is well defined due to the above remarks, it is injective, and it follows that it is an \(R\)-homomorphism and that \(g(\phi(m_3))=g(y)=m_3\), thus \(g \circ \phi = \text{id}_{M3}\).</p> <p><strong>(iii)</strong>\(\Rightarrow\)<strong>(i)</strong> Assume the existence of a function \(\psi\) as described in (iii). Let \(h:M_2 \mapsto M_2, h = \phi \circ g\). It is clearly an \(R\)-endomorphism on \(M_2\), and it is idempotent: \(\phi \circ g \circ \phi \circ g = \phi \circ g\). Note that \(\ker(g) \subset \ker(h)\), and if \(h(m_2) = 0\) this implies that \(\phi(g(m_2)) = 0\), however recall that \(\phi\) is injective, and so \(g(m_2) = 0\) thus \(m_2 \in \ker(g)\), implying that \(\ker(g)=\ker(h)\). We can then write \(M_2 = \text{Im}(h)\oplus\ker(h)=\text{Im}(h)\oplus\ker(g)\), and so the sequence is split exact.</p> <p>Now, assume that we have a split exact sequence. We wish to show that \(M_2 \cong M_1 \oplus M_3\). Define \(\tau:M_1 \oplus M_3 \mapsto M_2,\tau((m_1,m_3))=\tau((m_1,(g \circ \phi)(m_3))) = f(m_1) + \phi(m_3)\). We claim that \(\tau\) is an \(R\)-isomorphism. Indeed, it is easy to see that \(\tau\) is an \(R\)-homomorphism, as it is defined in terms of the sum of two \(R\)-homomorphisms. If \(\tau((m_1,m_3)) = 0\), this implies that</p> \[f(m_1) + \phi(m_3) = 0 \Rightarrow g(f(m_1) + \phi(m_3)) = 0 \Rightarrow m_3 = 0\] <p>since \(\ker(g) = \text{Im}(f)\) and \(g \circ \phi = \text{id}_{M_3}\). Since \(f\) is injective, this implies that \(m_1 = 0\), and so \(\ker(\tau) = \{0\}\), i.e., it is injective. Given any \(m_2 \in M_2\), we know that we can write it uniquely as \(m_2 = f(m_1) + y = x + y\), where \(m_1 \in M_1,y \in N,x \in \ker(g)\). Let \((m_1,g(y)) \in M_1 \oplus M_3\), and note that</p> \[\tau((m_1,g(y))) = f(m_1) + \phi(g(y)) = f(m_1) + y = m_2\] <p>since \(\phi(g(y))=y\) by definition as \(y \in N\). Thus \(\tau\) is surjective, implying that it is an \(R\)-isomorphism and that \(M_2 \cong M_1 \oplus M_3\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="sufficient-condition-for-split-exact-sequence">Sufficient condition for split exact sequence</h2> <p><strong>Lemma 2:</strong> Given an exact sequence of the form given in the previous diagram, if \(M_3\) is a free \(R\)-module, then the sequence is split exact.</p> <p><strong>Proof:</strong> If \(M_3\) is free, then there exists a basis \(B = \{x_i\}_{i \in \mathcal{I}}\) for some arbitrary set of indices \(\mathcal{I}\). Since \(g\) is surjective, for every \(x_i\) there exists an \(m_i \in M_2\) such that \(g(m_i) = x_i\), thus define \(h:M_3\mapsto M_2\) such that \(\forall i \in \mathcal{I}:h(x_i) = m_i\), and since for every \(x \in M_3\) can be written as \(x = \sum_j \alpha_{i_j}x_{i_j}\), define \(h(x) = \sum_j \alpha_{i_j}h(x_{i_j})\). It follows that \(h\) is indeed an \(R\)-homomorphism by construction, and that</p> \[g(h(x)) = g(\sum_j \alpha_{i_j}h(x_{i_j}))=\sum_j \alpha_{i_j}g(h(x_{i_j}))=\sum_j \alpha_{i_j}x_{i_j} = x\] <p>thus \(g \circ h = \text{id}_{M_3}\), and by the previous Lemma we conclude that the sequence is split exact.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="the-main-result">The main result</h2> <p>Our strategy here is relatively simple: we will proceed via induction on the rank of the \(R\)-module. We will try to express any submodule as a direct sum of free modules with limited rank, and then conclude that the submodule itself also must have limited rank. This is where the exact sequences will come into play: we will build a natural exact sequence from the fact that our module is free with finite rank, and from this we’ll use the previous lemmas together with induction to conclude the desired result.</p> <p>Recall that any \(R\)-module over a unitary commutative ring with finite basis has the rank invariant property, i.e., any basis must have the same size. We now prove the main result.</p> <p><strong>Theorem</strong>: Let \(D\) be a principal ideal domain and let \(M\) be a free \(D\)-module with finite basis and rank \(n\), then any \(D\)-submodule is also free with rank at most \(n\).</p> <p><strong>Proof:</strong> The proof will be by induction on \(n\). If \(n = 1\), then \(M \cong D\), and since \(D\) is a PID, it means that any \(D\)-submodule of \(D\) is an ideal of \(D\), thus being generated by at most one element, i.e., each \(D\)-submodule is free and with rank at most \(1\). Now assume that if the rank of \(M\) is strictly smaller than \(n\), then any \(D\)-submodule is also free with rank at most the rank of \(M\).</p> <p>Let the rank of \(M\) be \(n\). It follows that \(M \cong D^{(n)}\), i.e., \(M\) is isomorphic to the direct sum of \(D\) with itself \(n\) times. Let such isomorphism be denoted by \(\phi\), and let \(\pi_1:D^{(n)} \mapsto D,\pi_1((r_1,...,r_n))=r_1\) be the projection onto \(D\) w.r.t. the first element of the \(n\)-tuple. Let \(\{x_1,...,x_n\}\) be a basis for \(M\), and assume without loss of generality that \(\phi(x_i)=e_i\), where \(e_i\) denotes the \(n\)-tuple with \(1\) on the \(i\)-th position and \(0\) elsewhere. Define \(\psi:M\mapsto D,\psi = \pi_1 \circ \phi\), and let \(N \leq M\) be a \(D\)-submodule and \(L = \langle x_2,...,x_n\rangle\) be the \(D\)-submodule of \(M\) generated by the elements \(x_2,...,x_n\).</p> <p>Let \(\tau\) denote the restriction of \(\psi\) onto \(N\), and note that \(\text{Im}(\tau)\leq D\) by definition, thus by the induction hypothesis \(\text{Im}(\tau)\) is a free \(D\)-submodule of \(D\) with rank at most \(1\) (since \(D\) has rank \(1\)). Also, note that \(\ker(\tau)\leq L\), since \(\ker(\psi) = L\), and since \(L\) is a free \(D\)-module of rank \(n-1\), it follows that the rank of \(\ker(\tau)\) is at most \(n-1\). Now consider the following diagram:</p> \[0 \mapsto \ker(\tau) \mapsto N \mapsto_{\tau} \text{Im}(\tau) \mapsto 0\] <p>Note that the diagram is clearly an exact sequence, where the mapping between \(\ker(\tau)\) and \(N\) is merely the identity in \(N\). Also, we’ve concluded that \(\text{Im}(\tau)\) is a free \(D\)-module, thus by Lemma \(2\) it follows that the sequence is split exact, and then by Lemma \(1\) it follows that \(N \cong \ker(\tau) \oplus \text{Im}(\tau)\). However \(\ker(\tau) \oplus \text{Im}(\tau)\) is a free \(D\)-module of rank at most \(n\), and thus \(N\) is a free \(D\)-module of rank at most \(n\), which concludes the proof.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>I found the proof of the main theorem to be extremely interesting: although the exact sequence that we create arises naturally, having the idea of building such sequence in the first place is really clever, and in first sight the properties of split exact sequences given by Lemmas 1 and 2 are not at all obvious. However, after trying to prove the theorem by myself, I believe that the meaning of these results becomes much more clear and natural.</p> <p>This result also shows how much more complicated things can become once one generalizes the notion of a vector space. When dealing with finite dimensional vector spaces over a field, the main theorem is way simpler to prove: it is easy to show that any linearly independent set of vectors can either be extended to a bigger linearly independent set of vectors or it already is a basis (in order to prove this one needs invertibility, i.e., the ring must be a division ring). From this, given an arbitrary subspace, any set of linearly independent vectors must be smaller than the dimension of the entire space, and thus we can choose an arbitrary non-zero vector of the subspace and extend it to a basis of the subspace, hence concluding that the rank of the subspace is at most the rank of the space.</p>]]></content><author><name></name></author><category term="math"/><category term="rings-and-modules"/><summary type="html"><![CDATA[another interesting result from the course on rings and modules]]></summary></entry><entry><title type="html">when will an endomorphism ring be a field?</title><link href="https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings/" rel="alternate" type="text/html" title="when will an endomorphism ring be a field?"/><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings/"><![CDATA[<p>I’m currently taking a course on Rings and Modules at UFMG, and this week I came across an interesting result that I’ve decided to share here.</p> <p>Let \(R\) be a ring with unity. We wish to show that if \(R\) is commutative and if \(M\) is a simple \(R\)-module, then the ring of \(R\)-endomorphisms \(\text{End}_R(M)\) is a field.</p> <h2 id="isomorphisms-between-rings-of-endomorphisms">Isomorphisms between rings of endomorphisms</h2> <p>We will first prove an auxiliary result that turns out to be quite useful.</p> <p><strong>Lemma 1:</strong> Let \(R\) be a unitary ring, and let \(M,N\) be two isomorphic \(R\)-modules. Then \(\text{End}_R(M)\) and \(\text{End}_R(N)\) are isomorphic as rings.</p> <p><strong>Proof:</strong></p> <p>Let \(\phi:M\mapsto N\) be the \(R\)-isomorphism between \(M\) and \(N\). Define \(\psi:\text{End}_R(M)\mapsto \text{End}_R(N)\), where \(\psi(f)(n) = \phi(f(\phi^{-1}(n)))\), for all \(f \in \text{End}_R(M),n \in N\). We claim that \(\psi\) is a ring isomorphism.</p> <p>If \(f,g \in \text{End}_R(M)\), then</p> \[\psi(f+g)(n)=\phi((f+g)(\phi^{-1}(n)))\] \[=\phi(f(\phi^{-1}(n))+g(\phi^{-1}(n)))\] \[= \phi(f(\phi^{-1}(n)))+\phi(g(\phi^{-1}(n)))\] \[= \psi(f)(n) + \psi(g)(n),\forall n \in N\] <p>since \(\phi\) is itself an endomorphism, and thus additive over \(M\).</p> <p>Also</p> \[\psi(f\circ g)(n) = \phi(f(g(\phi^{-1}(n))))\] \[=\phi(f(\phi^{-1}(\phi(g(\phi^{-1}(n))))))\] \[=\phi(f(\phi^{-1}(\psi(g)(n))))\] \[=\psi(f)(\psi(g)(n)) = (\psi(f) \circ \psi(g))(n),\forall n \in N\] <p>therefore \(\psi\) is a ring homomorphism.</p> <p>If \(f \in \ker(\psi)\), we have that \(\forall n \in N: \psi(f)(n) = 0 \Rightarrow \phi(f(\phi^{-1}(n))) = 0\). Since \(\phi\) is a \(R\)-module isomorphism, this implies that \(\forall n \in N: f(\phi^{-1}(n)) = 0\), but again \(\phi\) is an isomorphism, thus its inverse is also surjective, which implies that \(\forall m \in M: f(m) = 0\), and so \(f = 0 \Rightarrow \ker(\psi)=\{0\}\), i.e., \(\psi\) is injective.</p> <p>Given \(g \in \text{End}_R(N)\), define \(f(m) = \phi^{-1}(g(\phi(m)))\). Note that \(f\) is an endomorphism, since \(\phi\) and \(g\) both are. Also, note that</p> \[\psi(f)(n) = \phi(\phi^{-1}(g(\phi(\phi^{-1}(n))))) = g(n),\forall n \in N\] <p>thus \(\psi(f) = g\), and so \(\psi\) is surjective. The previous points show that \(\psi\) is indeed a ring isomorphism, which concludes the proof.</p> <p>\(\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\)</p> <h2 id="endomorphisms-of-commutative-rings">Endomorphisms of commutative rings</h2> <p>We now look at the ring of \(R\)-homomorphisms of a commutative unitary ring \(R\).</p> <p><strong>Lemma 2:</strong> Let \(R\) be a commutative ring with unity. The following statements hold:</p> <ol> <li>The rings \(R\) and \(\text{End}_R(R)\) are isomorphic via \(\phi:\text{End}_R(R)\mapsto R,\phi(f)=f(1),\forall f \in \text{End}_R(R)\).</li> <li>If \(I\) is an ideal of \(R\), then \(R/I\) is isomorphic to \(\text{End}_R(R/I)\).</li> </ol> <p><strong>Proof:</strong></p> <p>For the first statement, let \(f,g \in \text{End}_R(R)\), and note that</p> \[\phi(f+g)=(f+g)(1)=f(1)+g(1)=\phi(f)+\phi(g)\] \[\phi(f\circ g) = f(g(1)) = f(1\cdot g(1))=f(1)g(1)=\phi(f)\cdot\phi(g)\] <p>thus \(\phi\) is a ring homomorphism. If \(f \in \ker(\phi)\), we have that \(f(1) = 0\), but note that \(\forall a \in R:f(a) = a\cdot f(1)\), since \(f\) is a \(R\)-homomorphism, thus if \(f(1)=0\), this implies that \(f = 0\), and so \(\phi\) is injective. Moreover, given any \(a \in R\), we can define \(f(1)=a\), and \(f(b)=b\cdot a\), which yields that \(\phi(f)=a\) and that \(f\) is a \(R\)-homomorphism. Therefore \(\phi\) is a ring isomorphism.</p> <p>For the second statement, note that since \(R\) is commutative and \(I\) is a bilateral ideal of \(R\), \(R/I\) is a ring, and thus \(R/I\) can be viewed as an \(R/I\)-module. On the other hand, since \(I\) is an ideal and \(I\) is an \(R\)-module, \(R/I\) is also an \(R\)-module. Thus, if \(f \in \text{End}_{R/I}(R/I)\), note that</p> \[f(a \cdot (b+I))=f((a+I)(b+I)) = (a+I)f(b+I)=a\cdot f(b+I)\] <p>thus \(f \in \text{End}_{R}(R/I)\). Conversely, if \(f \in \text{End}_{R}(R/I)\), we have that</p> \[f((a+I)(b+I))=f(ab+I)=f(a(b+I))=af(b+I)=(a+I)f(b+I)\] <p>and so \(f \in \text{End}_{R}(R/I)\), implying that \(\text{End}_{R/I}(R/I) = \text{End}_{R}(R/I)\). Using this fact and the previous result, we conclude that \(R/I\) and \(\text{End}_{R}(R/I)\) are isomorphic rings.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="putting-it-all-together">Putting it all together</h2> <p>Recall that an \(R\)-module \(M\) is called simple if it has no proper non-trivial submodules, i.e., its only submodules are \(\{0\}\) and \(M\). We know that this is equivalent to two things: first that \(M\) must be cyclic, and every non-zero element of \(M\) must be a generator, and second that \(M\) is \(R\)-isomorphic to \(R/I\), where \(I\) is some maximal left-ideal of \(R\). We are now ready to prove our main result.</p> <p><strong>Theorem:</strong> Let \(R\) be a commutative ring with unity, and \(M\) be a simple \(R\)-module. Then \(\text{End}_{R}(M)\) is a field.</p> <p><strong>Proof:</strong></p> <p>Since \(M\) is simple, it follows that it is \(R\)-isomorphic to \(R/I\), where \(I\) is a maximal left-ideal of \(R\), but since \(R\) is commutative, this implies that \(I\) is a maximal bilateral ideal. From Lemma 1, it follows that \(\text{End}_{R}(M)\) and \(\text{End}_{R}(R/I)\) are isomorphic rings, and from Lemma 2 it follows that \(\text{End}_{R}(R/I)\) is isomorphic to \(R/I\) as rings, so \(\text{End}_{R}(M)\) is isomorphic to \(R/I\) as rings. However, note that since \(I\) is a maximal bilateral ideal, \(R/I\) is a field, and thus \(\text{End}_{R}(M)\) is itself a field.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>The proof of this result is quite simple due to it being mainly a series of straightforward algebraic manipulations. However, I still think that it portrays some interesting techniques.</p> <p>Lemma 1 essentially provides a way of converting a module homomorphism between two modules to a ring homomorphism between two rings that are closely related to the original modules. This surely has limited applications, however in our case it is precisely what we needed: we start by assuming that \(M\) is simple, and thus it is isomorphic to \(R/I\) as a module, however due to the fact that \(I\) is maximal we also know that \(R/I\) is a field. We then need a way of connecting this module isomorphism to a ring isomorphism, and this is precisely what Lemma 1 does. Lemma 2 on the other hand is just a bundle of simple observations that follow from basic properties of unitary commutative rings, and so it itself isn’t of much interest, however combined with Lemma 1 it yields the desired result. As for the main theorem itself, it yields a more powerful version of Schur’s Lemma: function composition for simple modules over commutative rings is itself commutative.</p>]]></content><author><name></name></author><category term="math"/><category term="rings-and-modules"/><summary type="html"><![CDATA[an interesting result from the course on rings and modules]]></summary></entry></feed>