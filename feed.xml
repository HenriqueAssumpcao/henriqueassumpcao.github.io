<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://henriqueassumpcao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://henriqueassumpcao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-12T15:32:12+00:00</updated><id>https://henriqueassumpcao.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">completion of a metric space</title><link href="https://henriqueassumpcao.github.io/blog/2024/metric_completion/" rel="alternate" type="text/html" title="completion of a metric space"/><published>2024-03-28T00:00:00+00:00</published><updated>2024-03-28T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/metric_completion</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/metric_completion/"><![CDATA[<p>I’m taking a course in Topology this semester, and I came across this interesting exercise about completions of metric spaces. We will show that any metric space can be extended to a complete metric space, i.e., a space where all Cauchy sequences converge.</p> <h2 id="some-basic-concepts">Some basic concepts</h2> <p>Let \((E,d)\) be a metric space, i.e., a set with a function \(d:E\times E\mapsto \mathbb{R}\) such that</p> <p>(i) \(d(x,y)= 0\) iff \(x =y\)</p> <p>(ii) \(d(x,y) = d(y,x)\) (Symmetry)</p> <p>(iii) \(d(x,y) \leq d(x,z) + d(z,x)\) (Triangle Inequality)</p> <p>We’ll denote a sequence of elements of \(E\) by \((x_n) \subset E\), and we recall that a sequence is Cauchy if for every \(\epsilon &gt; 0\), there exists some natural \(K\) such that</p> \[i,j &gt; K \Rightarrow d(x_i,x_j) &lt; \epsilon\] <p>It is immediate that any convergent sequence in \(E\) is Cauchy, however the converse is not true in general. Cauchy sequences are sequences that really want to converge to something, but that something may not be in \(E\), e.g., the sequence \(((1+1/n)^n) \subset \mathbb{Q}\) is Cauchy, however we know that it converges to Euler’s constant \(e\), which is irrational. Our goal here is to show that we can build a larger space where every Cauchy sequence converges to a point in that space, and in the case of \(\mathbb{Q}\) this will yield one of the possible constructions for the real numbers.</p> <p>Now recall that if \((E_1,d_1),(E_2,d_2)\) are metric spaces, a function \(f:E_1\mapsto E_2\) is called an isometry if</p> \[\forall x,y \in E_1: d_1(x,y) = d_2(f(x),f(y))\] <p>Note that from this definition it follows that an isometry is always injective and continuous, more specifically it is Lipschitz continuous with constant 1. Sometimes isometries are also required to be bijective, but we will make no such requirement. The main point here is that an isometry is a distance-preserving injective map, and thus in some it preserves the metric structure of the domain in its image. Our goal is not only to construct a complete space from \(E\), but do this in a way that guarantees that \(E\) is contained in this larger space via an isometry.</p> <h2 id="the-space-of-cauchy-sequences">The space of Cauchy sequences</h2> <p>I think that the hardest part of this exercise is probably thinking about a good candidate for the space to complete \(E\). We’ll consider the following set</p> \[C[E] = \{(x_n) \subset E|(x_n)\text{ is Cauchy}\}\] <p>We’ll equip this set with the following function</p> \[D((x_n),(y_n)) = \lim_{n\rightarrow \infty}d(x_n,y_n)\] <p>After seeing this construction, it is not hard to believe that something related to this set will be our completion, however thinking about this construction in the first place is something that I found to be quite challenging. The intuition here is that \(D\) is measuring how close the sequences are getting as \(n\) gets arbitrarily large. We first note that \(D\) is almost a metric, since it follows by definition that \(D\) is symmetric and respects the triangle inequality. However, it is not true that if \(D((x_n),(y_n)) = 0\) then \((x_n),(y_n)\) are the same, since any pair of distinct sequences that “want” to converge to the same point will have distance zero. For example, we can consider \(C[\mathbb{Q}]\) and note that the sequences \((1/n)\) and \((-1/n)\) both converge to zero, and thus</p> \[\lim_{n\rightarrow \infty}|1/n-1/n| = 0\] <p>however they are clearly distinct. Formally, \(D\) is a pseudo-metric, and one classical basic exercise in analysis shows that any space with a pseudo-metric can be naturally transformed into a metric space. In order for us to do that, we define the equivalence relation \(\sim\) on \(C[E]\) given by</p> \[(x_n) \sim (y_n) \Longleftrightarrow D((x_n),(y_n)) = 0\] <p>Thus, we can consider the space</p> \[\overline{E} := C[E]/\sim = \{[(x_n)]|(x_n) \in C[E]\}\] <p>where \([(x_n)]\) denotes the equivalence class of \((x_n)\) w.r.t. \(\sim\). Intuitively, this means that we’ve aggregated all Cauchy sequences that “want” to converge to the same point into a single set, and now we are treating this set as a point in itself. From now on, we’ll denote \((x_n) \in C[E]\) by \(\overline{x}\), and we note that the function</p> \[\overline{D}([\overline{x}],[\overline{y}]) := D(\overline{x},\overline{y}),\overline{x} \in [\overline{x}],\overline{y} \in [\overline{y}]\] <p>is well-defined (follows from the triangle inequality), i.e., we can evaluate the distance between \([\overline{x}]\) and \([\overline{y}]\) by evaluating the distance between any representative of the respective equivalence classes w.r.t. \(D\).</p> <h2 id="constructing-an-isometry">Constructing an Isometry</h2> <p>Now we’ll explicitly build an isometry between \(E\) and \(\overline{E}\). Consider the following map</p> \[\begin{split} \iota:E&amp;\mapsto\overline{E}\\ \iota(x) &amp;= [(x,x,...,x,...)] \end{split}\] <p>That is, we map each point \(x \in E\) to the equivalence class of the constant sequence with each element equal to \(x\). Such sequence is clerly Cauchy, and it converges to \(x\), thus \(\iota(x)\) is the set of all sequences that converge to \(x\). Now, we note that</p> \[\begin{split} \overline{D}(\iota(x),\iota(y)) &amp;= D((x,x,...,x,...),(y,y,...,y,...))\\ &amp;= \lim_{n\rightarrow \infty}d(x,y) = d(x,y) \end{split}\] <p>hence \(\iota\) is indeed an isometry, i.e., it is injective, continuous, and preserves the distances in \(E\). This means that, in a very concrete and well-defined sense, \(E\) is contained in \(\overline{E}\), and so our construction so far makes some sense in the context of metric spaces. We now prove an additional important result.</p> <p><strong>Lemma:</strong> \(\iota(E)\) is a dense subset of \(\overline{E}\). In other words, every point on \(\overline{E}\) can be approximated by a sequence of points in \(\iota(E)\).</p> <p><strong>Proof:</strong> Let \([\overline{x}] \in \overline{E}\) be an element of \(\overline{E}\), and note that by definition \(\overline{x}\) is a Cauchy sequence in \(E\), hence for every \(N \in \mathbb{N}\), we can find some \(K_N \in \mathbb{N}\) such that</p> \[i,j \geq K_N \Rightarrow d(x_i,x_j) &lt; 1/2N\] <p>In particular, we have that \(d(x_i,x_{K_N}) &lt; 1/2N\) for every \(i &gt; K\), which yields</p> \[\lim_{n\rightarrow \infty} d(x_n,x_{K_N}) \leq 1/2N &lt; 1/N \Rightarrow \overline{D}([\overline{x}],\iota(x_{K_N})) &lt; 1/N\] <p>Thus, we consider the sequence \((\iota(x_{N_K}))_{K \in \mathbb{N}} \subset \overline{E}\). If we fix \(\epsilon &gt; 0\), we can always find some natural \(N\) such that \(1/N &lt; \epsilon\), and so</p> \[i &gt; K_N \Rightarrow \overline{D}([\overline{x}],\iota(x_{K_N})) &lt; 1/N &lt; \epsilon\] <p>which shows that \((\iota(x_{N_K}))\) converges to \([\overline{x}]\) in \(\overline{E}\), implying that \(\iota(E)\) is dense in \(\overline{E}\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <p>This means that, not only is \(E\) contained in \(\overline{E}\) in a way that preserves distances, but also that we can approximate any element of \(\overline{E}\) by some sequence of elements of \(E\).</p> <h2 id="the-main-result">The main result</h2> <p>We are now ready to show that \(\overline{E}\) is complete, i.e., if we have a Cauchy sequence \(([\overline{x}^{(k)}])_{k \in \mathbb{N}}\) in \(\overline{E}\), then it must converge to some \([\overline{x}]\) in \(\overline{E}\). Concretely, this means that as \(k\) gets arbitrarily large, the point to which the Cauchy sequences in \([\overline{x}^{(k)}]\) “want” to converge to gets arbitrarily close to the point that \([\overline{x}]\) “wants” to converge to.</p> <p><strong>Theorem:</strong> If \((E,d)\) is a metric space, than the space \((\overline{E},\overline{D})\) is a complete metric space. Moreover, there is an isometry \(\iota\) between \(E\) and \(\overline{E}\) such that \(\iota(E)\) is dense in \(\overline{E}\).</p> <p><strong>Proof:</strong> The trickiest part of this proof is finding a candidate for the convergent point of the sequence \(([\overline{x}^{(k)}])\). First, we note that \(([\overline{x}^{(k)}])\) converges to \([\overline{x}]\) iff \((\overline{x}^{(k)})\) converges to \(\overline{x}\) w.r.t. \(D\), for some elements \(\overline{x}^{(k)},\overline{x}\) of the respective equivalence classes, thus we only need to construct a Cauchy sequence \(\overline{x}\) in \(E\) such that for any \(\epsilon &gt; 0\) there exists some \(k \in \mathbb{N}\) such that</p> \[i &gt; k \Rightarrow \lim_{n\rightarrow \infty} d(x^{(i)}_n,x_n) &lt; \epsilon\] <p>where \(\overline{x}^{(i)} = (x_n^{(i)})\). In order to do this, we first use that each \(\overline{x}^{(k)}\) is itself a Cauchy sequence, i.e., for every \(k \in \mathbb{N}\), we can find some \(n_k \in \mathbb{N}\) such that</p> \[i,j \geq n_k \Rightarrow d(x^{(k)}_i,x^{(k)}_j) &lt; 1/k\] <p>hence our candidate will be the sequence</p> \[\overline{x} = (x^{(1)}_{n_1},x^{(2)}_{n_2},...,x^{(k)}_{n_k},...) = (x^{(k)}_{n_k})_{k \in \mathbb{N}}\] <p>We now need to show that this sequence is Cauchy, and also that \(\overline{x}^{(k)}\) converges to it. First, let \(\delta &gt; 0\), and thus we can find some \(k_1 \in \mathbb{N}\) such that \(1/k_1 &lt; \delta/6\), and we can also find some \(k_2 \in \mathbb{N}\) such that</p> \[i,j &gt; k_2 \Rightarrow D(\overline{x}^{(i)},\overline{x}^{(j)}) = \lim_{n\rightarrow \infty} d(x^{(i)}_n,x^{(j)}_n) &lt; \delta/6\] <p>since \((\overline{x}^{(k)})\) is also a Cauchy sequence. Now let \(k =\max(k_1,k_2)\), thus if \(i,j &gt; k\) we have</p> \[d(x^{(i)}_{n_i},x^{(j)}_{n_j}) \leq d(x^{(i)}_{n_i},x^{(i)}_{n}) + d(x^{(i)}_{n},x^{(j)}_{n}) + d(x^{(j)}_{n},x^{(j)}_{n_j})\] <p>thus if we take \(n &gt; \max(n_i,n_j)\), we get that</p> \[\begin{split} d(x^{(i)}_{n_i},x^{(i)}_{n}) &amp;&lt; 1/i &lt; 1/k &lt; \delta/6\\ d(x^{(j)}_{n_j},x^{(j)}_{n}) &amp;&lt; 1/j &lt; 1/k &lt; \delta/6\\ \end{split}\] <p>Hence if \(n &gt; \max(n_i,n_j)\), this implies that</p> \[\begin{split} d(x^{(i)}_{n_i},x^{(j)}_{n_j}) &amp;&lt; \delta/3 + d(x^{(i)}_{n},x^{(j)}_{n})\\ \Rightarrow d(x^{(i)}_{n_i},x^{(j)}_{n_j}) &amp;= \lim_{n\rightarrow \infty} d(x^{(i)}_{n_i},x^{(j)}_{n_j}) \leq \delta/3 + \lim_{n\rightarrow \infty}d(x^{(i)}_{n},x^{(j)}_{n})\\ &amp;\leq \delta/3 + \delta/6 &lt; \delta \end{split}\] <p>and since \(\delta\) was arbitrary, this shows that \(\overline{x}\) is Cauchy. Now fix \(\epsilon &gt; 0\), and let \(k_1 \in \mathbb{N}\) be such that \(1/k_1 &lt; \epsilon/4\), thus</p> \[n &gt; n_{k_1} \Rightarrow d(x^{(k_1)}_n,x^{(k_1)}_{n_{k_1}}) &lt; 1/k_1 &lt; \epsilon/4\] <p>Since \(\overline{x}\) is itself Cauchy, there exists some \(k_2 \in \mathbb{N}\) such that</p> \[i &gt; k_2 \Rightarrow d(x^{(i)}_{n_i},x^{(k_2)}_{n_{k_2}}) &lt; \epsilon/4\] <p>Let \(k = \max(k_1,k_2)\), thus we obtain</p> \[\begin{split} n &gt; n_k &amp;\Rightarrow d(x^{(k)}_n,x^{(k)}_{n_k}) &lt; \epsilon/4\\ i &gt; k &amp;\Rightarrow d(x^{(i)}_{n_i},x^{(k)}_{n_k}) &lt; \epsilon/4 \end{split}\] <p>Combining these two inequalities, if \(j &gt; \max(k,n_k)\), we get that</p> \[\begin{split} d(x^{(k)}_j,x^{(j)}_{n_j}) &amp;\leq d(x^{(k)}_j,x^{(k)}_{n_k}) + d(x^{(j)}_{n_j},x^{(k)}_{n_k}) &lt; \epsilon/2\\ \Rightarrow D(\overline{x}^{(k)},\overline{x}) &amp;= \lim_{j \rightarrow \infty} d(x^{(k)}_j,x^{(j)}_{n_j}) \leq \epsilon/2 &lt; \epsilon \end{split}\] <p>and thus we can conclude that \(([\overline{x}^{(k)}])\) converges to \([\overline{x}]\), i.e., \(\overline{E}\) is complete.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>This result shows that we can indeed build a new space from \(E\) that is a complete metric space that “contains” \(E\) in a meaningful way, i.e., there is a dense subset of this new space that is isometric to \(E\). It is also easy to check that \(\iota(E) = \overline{E}\) iff \(E\) is complete, and through some extra work it can also be shown that \(\overline{E}\) is unique up to isometry, i.e., any other space \(\overline{E}'\) that contains a dense subset that is isometric to \(E\) is itself isometric to \(\overline{E}\).</p> <p>This gives an interesting way of thinking about the real numbers: an irrational number such as \(\pi\) can be viewed as the set of sequences of rational numbers that are getting arbitrarily close together and converging to something that isn’t rational, namely \(\pi\) itself. From the “perspective” of the rational numbers, the real numbers are nothing more than bundles of sequences that are getting really close together and “wanting” to converge to something. Sometimes this something turns out to be a rational number, but sometimes it turns out not to be, and when this is the case we simply identify this “thing” that isn’t rational with the set of sequences that are getting arbitrarily close to it.</p>]]></content><author><name></name></author><category term="math"/><category term="topology"/><summary type="html"><![CDATA[how to complete a metric space]]></summary></entry><entry><title type="html">the schur basis and some other decompositions</title><link href="https://henriqueassumpcao.github.io/blog/2024/matrix_decomp/" rel="alternate" type="text/html" title="the schur basis and some other decompositions"/><published>2024-02-28T00:00:00+00:00</published><updated>2024-02-28T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2024/matrix_decomp</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2024/matrix_decomp/"><![CDATA[<p>A few months ago I was studying Bhatia’s Matrix Analysis book while I stumbled upon a pretty basic result that, for some reason, I had never proven before in any of my linear algebra classes: that every matrix over the complex numbers has an orthogonal basis such that it is in an upper triangular form (called the Schur basis). In this post I’ll prove this result, and then use it to prove that an arbitrary family of commuting matrices has a commom Schur basis, and also talk a bit about other interesting related results.</p> <h2 id="triangular-and-normal-matrices">Triangular and Normal matrices</h2> <p>Let \(V\) be a \(n\)-dimensional vector space over the complex numbers, and assume we have a matrix \(T\) in upper-triangular form, where we denote the \(ij\)-th element of \(T\) by \(T_{ij}\). The determinant of a triangular matrix is just the product of its diagonal elements, since any permutation other than the identity results in at least one zero appearing in the product of elements given by the determinant formula. Now note that</p> \[\varphi(T,\lambda) = \det(T - \lambda I)\] <p>where \(\varphi\) denotes the characteristic polynomial of \(T\), and since \(T - \lambda I\) is also upper triangular, we get that</p> \[\varphi(T,\lambda) = (T_{11} - \lambda)\cdot...\cdot(T_{nn} - \lambda)\] <p>and hence all of the eigenvalues of \(T\) are in its diagonal entries.</p> <p>We now prove that for any linear operator \(A\) on \(V\) there exists an orthogonal basis in which the matrix of such function is upper triangular. Note that this is equivalent to saying that there is an orthogonal basis \(\{v_1,...,v_n\}\) of \(V\) such that \(Av_i \in \text{span}_{\mathbb{C}}(v_1,...,v_i)\), and we can prove this via a simple induction on the dimension \(n\) of \(V\). If \(n = 1\) then any normalized non-zero vector satisfies the desired property. For an arbitrary \(n\), note that since \(\mathbb{C}\) is algebraically closed we can always find an eigenvalue \(\lambda\) with a non-zero eigenvector \(v_1\), and thus decompose the space as</p> \[V = \mathbb{C}v_1 \oplus (\mathbb{C}v_1)^\perp\] <p>where \(W = (\mathbb{C}v_1)^\perp\) is the \((n-1)\)-dimensional orthogonal complement of the span of \(v_1\). If we consider the orthogonal projection \(P\) onto \(W\), the function \(PA\) is a linear operator on \(W\), and thus by the inductive hypothesis we can find a basis \(\{v_2,...,v_n\}\) for \(W\) such that \((PA)(v_i) \in \text{span}_{\mathbb{C}}(v_2,...,v_i)\). Now if we consider \(\{v_1,...,v_n\}\), it is an orthogonal basis for \(V\), and since we can decompose the identity \(I = P' + P\), where \(P'\) is the projection onto the span of \(v_1\), we have that for every \(v_k\)</p> \[Av_k = ((P'+P)A)(v_k) = (P'A)(v_k) + (PA)(v_k)\] <p>By definition, \((P'A)(v_k)\) is in the span of \(v_1\), and by the induction hypothesis \((PA)(v_k)\) is in the span of \(v_2,...,v_k\), hence \(Av_k\) belongs to the span of \(v_1,...,v_k\), as desired. This basis is called the <em>Schur basis</em> of an operator, and from this it follows that for any matrix \(A\) there is an orthogonal matrix \(Q\) such that \(Q^*AQ\) is upper triangular.</p> <p>Recall that a linear operator \(A\) is called <em>normal</em> if it commutes with its adjoint, and in our case for any fixed basis the matrix of the adjoint is the conjugate transpose of the operator’s matrix. The spectral theorem states that an operator on a finite dimensional vector space over \(\mathbb{C}\) can be orthogonally diagonalized, i.e., written as \(Q\Lambda Q^*\) with \(Q\) orthogonal matrix, and \(\Lambda\) diagonal matrix of eigenvalues, if and only if, it is normal. This theorem can actually be regarded as a corollary of the existence of the Schur Basis. If we write \(A\) w.r.t. its Schur basis, we obtain</p> \[\begin{split} A &amp;= \begin{bmatrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n}\\ 0 &amp; a_{22} &amp; ... &amp; a_{2n}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 0 &amp; 0 &amp; ... &amp; a_{nn}\\ \end{bmatrix}\\ A^* &amp;= \begin{bmatrix} \overline{a_{11}} &amp; 0 &amp; ... &amp; 0\\ \overline{a_{12}} &amp; \overline{a_{22}} &amp; ... &amp; 0\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ \overline{a_{1n}} &amp; \overline{a_{2n}} &amp; ... &amp; \overline{a_{nn}}\\ \end{bmatrix} \end{split}\] <p>and since the matrices commute, if we look at the result of their diagonal products, it follows that any off-diagonal entry must be zero, and thus \(A\) and \(A^*\) must be diagonal matrices, and hence \(A\) can be orthogonally diagonalized.</p> <h2 id="nilpotent-matrices">Nilpotent matrices</h2> <p>An operator \(A\) on \(V\) is called <em>nilpotent</em> if there exists a non-negative integer \(r\) such that \(A^r = 0\), i.e., A composed with itself \(r\) times is equal to the zero map. From this definition it follows that any eigenvalue of \(A\) must be zero, and since we’ve observed that any operator on \(V\) has a Schur basis, it follows that there is a basis in which \(A\) is strictly upper triangular, i.e., all of its diagonal elements are zero. Conversely, any strictly upper triangular \(n \times n\) matrix satisfies \(N^n = 0\), and thus we get that an operator over a finite-dimensional vector space is nilpotent iff there is a basis in which its matrix is strictly upper triangular. From these facts and the Schur decomposition, it follows that for any matrix \(A\), there is an orthogonal matrix \(Q\) such that</p> \[Q^*AQ = D + N\] <p>where \(D\) is a diagonal matrix containing the eigenvalues of \(A\), and \(N\) is a nilpotent matrix in strict upper triangular form. This decomposition, however, is not unique, since the Schur basis is also not unique. We can, however, prove the uniqueness of the operators associated with \(D\) and \(N\), that is, we can prove that any operator \(A\) can be uniquely written as</p> \[A = D + N\] <p>where \(D\) is a <em>diagonalizable</em> operator (not necessarily in diagonal form), and \(N\) is a operator (again not necessarily in upper triangular form). Moreover, we can show that \(D\) and \(N\) are polynomials in \(A\), and hence they commute.</p> <p>For this, since we are assuming that \(V\) is a vector space over the complex numbers, we may assume that the minimal polynomial \(m_A\) of \(A\) is of the form</p> \[m_A(t) = (t-\lambda_1)^{r_1}\cdot\ldots\cdot(t-\lambda_k)^{r_k}\] <p>where each \(\lambda_i\) is an eigenvalue, and \(\lambda_i \neq \lambda_j\) if \(i \neq j\). From the spectral theorem, it follows that we can decompose \(V\) as</p> \[V = W_1 \oplus\ldots\oplus W_k\] <p>where \(W_i = \ker((T - \lambda_i I)^{r_i})\) is the <em>generalized eigenspace</em> associated with \(\lambda_i\), and we can consider the projections \(E_i\) in these spaces such that \(I = E_1 + ... + E_k\), and \(E_iE_j = \delta_{ij}E_i\). We can thus define the operator</p> \[D = \lambda_1 E_1 + ... + \lambda_k E_k\] <p>and we claim that \(D\) is diagonalizable. Indeed, this follows by simply observing that for any \(v \in V\), \(DE_iv = \lambda v\), hence \(W_i \subset \ker(D - \lambda I)\), and thus any basis of \(V\) given by the union of bases for each \(W_i\) is a basis of eigenvectors of \(D\). Now consider the operator</p> \[N = A - D\] <p>and note that \(N\) is a polynomial in \(A\), and so is \(D\), and thus they commute. Now since \(A = AE_1 + ... + E_k\), we have that</p> \[N = \sum_{i=1}^k (A-\lambda_i I)E_i\] <p>however, we recall that each \(E_i\) is a polynomial in \(A\), and hence commutes with \(A\), thus we have</p> \[\begin{split} N^2 &amp;= \sum_{i,j} (A-\lambda_i I)E_i(A-\lambda_j I)E_j \\ &amp;= \sum_{i,j} (A-\lambda_i I)(A-\lambda_j I)E_iE_j = \sum_i (A-\lambda_i I)^2E_i \end{split}\] <p>and in general we have that \(N^r = \sum_i (A-\lambda_i I)^rE_i\), then if \(r \geq \max(r_1,...,r_k)\), by definition of the minimal polynomial, we have that \(N^r = 0\), thus \(N\) is nilpotent. This shows that we can decompose any operator as a sum of a diagonalizable operator and a nilpotent operator. One way of thinking about this argument is that we are removing the diagonalizable part of \(A\) and then showing that this new operator is nilpotent, which makes sense since any non-zero nilpotent operator is not diagonalizable (since the only possible eigenvalue is zero).</p> <p>Now, we need to check that \(N\) and \(D\) are indeed unique (as operators, not as matrices). Assume that \(A = N + D = N' + D'\), and so</p> \[D - D' = N' - N\] <p>Since both \(N'\) and \(N\) are nilpotent, we can choose a sufficiently large \(r\) such that</p> \[(N' - N)^r = \sum_{i=0}^r (-1)^i {r \choose i}(N')^iN^{r-i} = 0\] <p>and so \(N' - N\) is also nilpotent. Note, however, that since both \(D\) and \(D'\) are diagonalizable and commute, they are simultaneously diagonalizable, and hence \(D-D'\) is also a diagonalizable operator, and so we have that a nilpotent operator is equal to a diagonalizable operator, however we know that the Schur basis of a nilpotent operator is a strictly upper triangular matrix, and that the schur basis of a diagonalizable operator is an eigenbasis, hence \(D-D' = N'-N = 0\), and we conclude that the operators are unique.</p> <h2 id="invariant-subspaces-and-commuting-matrices">Invariant subspaces and commuting matrices</h2> <p>If \(V\) is a vector space over \(\mathbb{C}\) of dimension \(n\), and \(A\) is a linear operator on \(V\), we can show that any \(A\)-invariant subspace of \(V\) contains an eigenvector of \(A\). Indeed, let \(W = \text{span}_{\mathbb{C}}(v_1,...,v_k)\) be a \(k\)-dimensional subspace of \(V\) such that \(AW \subset W\), let \(B = [v_1 \quad ... \quad v_k]\) be the \(n \times k\) matrix containing the basis of \(W\) in its columns. Since \(W\) is \(A\)-invariant, it follows that there must exist a \(k \times k\) matrix \(C\) such that</p> \[AB = BC\] <p>since \(AB = A[v_1\quad ... \quad v_k] = [Av_1 \quad ... \quad Av_k]\), and each \(Av_i\) can be expressed as a linear combination of \(v_1,...,v_k\). Since \(\mathbb{C}\) is algebraically closed (this argument works for any algebraically closed field), it follows that the matrix \(C\) itself has an eigenvector \(x\) with some eigenvalue \(\lambda\), hence</p> \[ABx = BCx = \lambda Bx\] <p>and thus \(Bx\) is an eigenvector of \(A\) (note that \(Bx\) must be non-zero since the vectors in \(B\) are linearly independent).</p> <p>This is quite useful because, now, if we consider two commuting linear operators \(A\) and \(B\), we can show that they have a common eigenvector, and, moreover, we can show that they have a common Schur basis. Let \(v\) be an eigenvector of \(A\) with eigenvalue \(\lambda\), and note that for any non-negative integer \(k\), we have that</p> \[B^kAv = \lambda B^kv \Rightarrow A(B^kv) = \lambda (B^kv)\] <p>hence \(B^kv\) is an eigenvector of \(A\) for every \(k \geq 0\). We can thus consider the space</p> \[W = \text{span}(v,Bv,B^2v,...)\] <p>and note that \(W\) is \(B\)-invariant, so there is some \(k\) for which \(B^kv\) is an eigenvector of \(B\), and thus a common eigenvector for \(A\) and \(B\) (not necessarily with the same eigenvalue). This can serve as a base case for showing that the result is true for any finite set of commuting matrices. Indeed, if we consider \(A_1,...,A_n\) as a set of commuting matrices, we can assume that there exists a common eigenvector \(v\) for \(A_1,...,A_{n-1}\), and using the same argument as before, we see that for any non-negative \(k\) that \(A_n^kv\) is a common eigenvector for all \(A_i\) with \(i &lt; n\). Again, we can consider the subspace</p> \[W = \text{span}(v,A_n^kv,A_n^{2k}v,...)\] <p>and note that it is \(A_n\)-invariant, and thus it also has an eigenvector of \(A_n\), which by definition must be an eigenvector for all \(A_1,...,A_{n-1}\), hence we have a common eigenvector of \(A_1,...,A_n\).</p> <p>Now, if \(A_1,...,A_n\) are a set of commuting matrices, we can show that they have a common Schur basis. Again, we proceed via induction on the dimension \(n\) of the space, and recall that we wish to show that there is a basis \(\{v_1,...,v_n\}\) of \(V\) such that</p> \[\forall i,j \in \{1,...,n\}:A_iv_j \in \text{span}_\mathbb{C}(v_1,...,v_j)\] <p>The base case of \(n=1\) is immediate. For the general case, we start with a common eigenvector \(v_1\) of \(A_1,...,A_n\) with respective eigenvalue \(\lambda_i\) for each \(A_i\), and decompose the space as</p> \[V = \mathbb{C}v_1 \oplus (\mathbb{C}v_1)^\perp\] <p>Note that if we consider a basis for \(V\) composed by a union of a basis for \(\mathbb{C}v_1\) and \((\mathbb{C}v_1)^\perp\), the matrices \(A_i\) will have the form</p> \[A_i = \begin{bmatrix} \lambda_i &amp; a_i\\ 0 &amp; A_{i}'\\ \end{bmatrix}\] <p>where \(a_i\) is a \(n-1\) dimensional line vector, \(0\) is the \(n-1\) dimensional zero vector, and \(A_i'\) is a \(n-1\times n-1\) block of \(A\). Now for two matrices \(A_i,A_j\), we have that</p> \[\begin{split} A_iA_j &amp;= \begin{bmatrix} \lambda_i\lambda_j &amp; c_{ij}\\ 0 &amp; A_{i}'A_{j}'\\ \end{bmatrix}\\ A_jA_i &amp;= \begin{bmatrix} \lambda_j\lambda_i &amp; d_{ji}\\ 0 &amp; A_{j}'A_{i}'\\ \end{bmatrix}\\ \end{split}\] <p>where \(c_{ij},d_{ji}\) are \(n-1\) dimensional line vectors (we can compute these explicitly, but we don’t really need that here). Since \(A_i,A_j\) commute, it follows that so do \(A_{i}',A_{j}'\), and thus \(A_1',...,A_n'\) are a finite set of commuting matrices operators on a \((n-1)\)-dimensional subspace of \(V\) (namely \((\mathbb{C}v_1)^\perp\)), and thus we obtain an orthogonal basis \(\{v_1,...,v_n\}\) for \(V\), and the same argument used to prove the existence of a Schur basis for a single matrix yields the desired result.</p> <h2 id="final-remarks">Final remarks</h2> <p>We’ve seen that any finite commuting set of matrices has a common Schur basis over the complex numbers, however this same result also implies that this set doesn’t need to be finite at all. This follows from the fact that, if \(\{A_i\}_{i \in \mathcal{I}}\) is an arbitrary family of commuting matrix with entries on \(\mathbb{C}\), then we can consider the <em>algebra</em> generated by such set. Recall that an algebra is just a ring which is also a module over some other ring, and in our case, an algebra is just a vector space that is closed w.r.t. matrix multiplication. Thus, the algebra generated by some set of matrices is nothing more than the set of all finite linear combinations of finite products between such matrices. For example, the algebra generated by some matrix \(A\) is just the set of all finite linear combinations of powers of \(A\). We say that an algebra is finite dimensional if it is so as a vector space, and hence the set of all \(n \times n\) matrices over \(\mathbb{C}\) is a finite dimensional algebra with dimension \(n^2\) (just take the matrices \(E_{ij}\) with only the \(ij\)-th entry non-zero and equal to one). Thus, any subalgebra of the full matrix algebra is also finite dimensional, and so the algebra generated by \(\{A_i\}_{i \in \mathcal{I}}\) must have a finite basis of matrices, and since the generating set is commutative, the entire algebra also is. Hence, we can find a common Schur basis for the matrices that form a basis of such algebra, and then any matrix in this algebra will also be in an upper triangular form. It also follows as a corollary that any family \(\{A_i\}_{i \in \mathcal{I}}\) of commuting normal matrices has a common orthogonal eigenbasis, i.e., there exists an orthogonal matrix \(Q\) such that \(Q^*A_iQ\) is diagonal for each \(i\).</p> <p>I’m currently studying matrix algebras and the Wedderburn-Artin theorems for my undergraduate thesis, so I intend on making more posts on connections between results in linear algebra and the study of matrix algebras and ring theory.</p>]]></content><author><name></name></author><category term="math"/><category term="linear-algebra"/><summary type="html"><![CDATA[a few cool results from linear algebra that I realized I had never proven before]]></summary></entry><entry><title type="html">free modules over principal ideal domains</title><link href="https://henriqueassumpcao.github.io/blog/2023/free_modules/" rel="alternate" type="text/html" title="free modules over principal ideal domains"/><published>2023-11-25T00:00:00+00:00</published><updated>2023-11-25T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2023/free_modules</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2023/free_modules/"><![CDATA[<p>We are interested in showing that free modules over principal ideal domains (PIDs) with finite basis behave in a similar fashion to vector spaces over fields: we wish to show that every submodule will also be free with a smaller basis. For this, we first tackle the notion of exact sequences of modules. Throughout this proof, \(R\) will denote a commutative ring with unity, and \(D\) will denote a PID. Consider the following diagram:</p> \[0 \mapsto M_1 \mapsto^f M_2 \mapsto^g M_3 \mapsto 0\] <p>where \(M_1,M_2,M_3\) are \(R\)-modules, and \(f,g\) are \(R\)-homomorphisms. We call this sequence exact if \(\ker(g)=\text{Im}(f)\). Given an exact sequence, we say that it is split exact if there exists some \(N\leq M_2: M_2 = \ker(g)\oplus N\), which also implies that \(M_2 = \text{Im}(f)\oplus N\). We will first prove two results related to split exact sequences, and then proceed to prove our main result.</p> <h2 id="a-characterization-of-split-exact-sequences">A characterization of split exact sequences</h2> <p><strong>Lemma 1:</strong> Given an exact sequence of the form given in the previous diagram, TFAE:</p> <ul> <li>The sequence is split exact.</li> <li>There exists \(\psi \in \text{Hom}_{R}(M_2,M_1)\) such that \(\psi \circ f = \text{id}_{M1}\).</li> <li>There exists \(\phi \in \text{Hom}_{R}(M_3,M_2)\) such that \(g \circ \phi = \text{id}_{M3}\).</li> </ul> <p>Under these conditions, we have that \(M_2 \cong M_1 \oplus M_3\).</p> <p><strong>Proof:</strong></p> <p><strong>(i)</strong>\(\Rightarrow\)<strong>(ii)</strong> Note that by the definition of our sequence, \(f\) is injective and \(g\) is surjective. Assume that \(M_2 = \text{Im}(f)\oplus N\), thus given any \(m_2 \in M_2\), we can write it uniquely as \(m_2 = f(m_1) + y\), where \(m_1 \in M_1,y \in N\), and note that \(m_1\) must be unique since \(f\) is injective. Thus, define \(\psi\) such that \(\psi(m_2) = \psi(f(m_1)+y)=m_1\). This function is well defined due to the above remarks, and note that the restriction of \(\psi\) onto \(\text{Im}(f)\) is a bijection, and also that \(\psi\) is an \(R\)-homomorphism: given \(m_2,m_2' \in M_2,\alpha \in R:\)</p> \[m_2+m_2' = f(m_1)+y+f(m_1')+y' = f(m_1+m_1') + y+y'\] \[\Rightarrow \psi(m_2+m_2')=\psi(m_2)+\psi(m_2')\] <p>and also that \(\psi(\alpha m_2) = \alpha \psi(m_2)\). Given any \(m_1 \in M_1\), note that \(\psi(f(m_1)) = m_1\) by definition, thus \(\psi \circ f = \text{id}_{M_1}\).</p> <p><strong>(ii)</strong>\(\Rightarrow\)<strong>(i)</strong> Now assume the existence of a function \(\psi\) as described in (ii). Let \(h:M_2 \mapsto M_2, h = f \circ \psi\). It is clearly an \(R\)-endomorphism on \(M_2\), and note that \(\text{Im}(h) \subset \text{Im}(f)\) by definition. On the other hand, given any \(m_1 \in M_1\), we have that \(f(m_1) = f(\psi(f(m_1))) = h(f(m_1)) \in \text{Im}(h)\), thus \(\text{Im}(h) = \text{Im}(f)\). Also, note that \(h\) is idempotent: \(f \circ \psi \circ f \circ \psi = f\circ \psi\), and so we can write \(M_2 = \text{Im}(h)\oplus\ker(h)=\text{Im}(f)\oplus\ker(n)\), and so the sequence is split exact.</p> <p><strong>(i)</strong>\(\Rightarrow\)<strong>(iii)</strong> Assume that \(M_2 = \ker(g)\oplus N\), and note that since \(g\) is surjective, given any \(m_3 \in M_3\), we can write it as \(m_3 = g(m_2)\), where \(m_2 \in M_2\). On the other hand, we can write each \(m_2\) uniquely as \(m_2 = x + y\), where \(x \in \ker(g),y \in N\), thus \(m_3 = g(y)\). We claim that this \(y\) is unique. Indeed, if there were another \(y' \in N\) s.t. \(m_3 = g(y')\), this would imply that \(g(y-y') = 0\), and so \(y-y' \in \ker(g)\cap N \Rightarrow y = y'\), since \(M_2\) is a direct sum. Thus, define \(\phi\) as \(\phi(m_3)=\phi(g(m_2))=\phi(g(y))=y\). This function is well defined due to the above remarks, it is injective, and it follows that it is an \(R\)-homomorphism and that \(g(\phi(m_3))=g(y)=m_3\), thus \(g \circ \phi = \text{id}_{M3}\).</p> <p><strong>(iii)</strong>\(\Rightarrow\)<strong>(i)</strong> Assume the existence of a function \(\psi\) as described in (iii). Let \(h:M_2 \mapsto M_2, h = \phi \circ g\). It is clearly an \(R\)-endomorphism on \(M_2\), and it is idempotent: \(\phi \circ g \circ \phi \circ g = \phi \circ g\). Note that \(\ker(g) \subset \ker(h)\), and if \(h(m_2) = 0\) this implies that \(\phi(g(m_2)) = 0\), however recall that \(\phi\) is injective, and so \(g(m_2) = 0\) thus \(m_2 \in \ker(g)\), implying that \(\ker(g)=\ker(h)\). We can then write \(M_2 = \text{Im}(h)\oplus\ker(h)=\text{Im}(h)\oplus\ker(g)\), and so the sequence is split exact.</p> <p>Now, assume that we have a split exact sequence. We wish to show that \(M_2 \cong M_1 \oplus M_3\). Define \(\tau:M_1 \oplus M_3 \mapsto M_2,\tau((m_1,m_3))=\tau((m_1,(g \circ \phi)(m_3))) = f(m_1) + \phi(m_3)\). We claim that \(\tau\) is an \(R\)-isomorphism. Indeed, it is easy to see that \(\tau\) is an \(R\)-homomorphism, as it is defined in terms of the sum of two \(R\)-homomorphisms. If \(\tau((m_1,m_3)) = 0\), this implies that</p> \[f(m_1) + \phi(m_3) = 0 \Rightarrow g(f(m_1) + \phi(m_3)) = 0 \Rightarrow m_3 = 0\] <p>since \(\ker(g) = \text{Im}(f)\) and \(g \circ \phi = \text{id}_{M_3}\). Since \(f\) is injective, this implies that \(m_1 = 0\), and so \(\ker(\tau) = \{0\}\), i.e., it is injective. Given any \(m_2 \in M_2\), we know that we can write it uniquely as \(m_2 = f(m_1) + y = x + y\), where \(m_1 \in M_1,y \in N,x \in \ker(g)\). Let \((m_1,g(y)) \in M_1 \oplus M_3\), and note that</p> \[\tau((m_1,g(y))) = f(m_1) + \phi(g(y)) = f(m_1) + y = m_2\] <p>since \(\phi(g(y))=y\) by definition as \(y \in N\). Thus \(\tau\) is surjective, implying that it is an \(R\)-isomorphism and that \(M_2 \cong M_1 \oplus M_3\).</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="sufficient-condition-for-split-exact-sequence">Sufficient condition for split exact sequence</h2> <p><strong>Lemma 2:</strong> Given an exact sequence of the form given in the previous diagram, if \(M_3\) is a free \(R\)-module, then the sequence is split exact.</p> <p><strong>Proof:</strong> If \(M_3\) is free, then there exists a basis \(B = \{x_i\}_{i \in \mathcal{I}}\) for some arbitrary set of indices \(\mathcal{I}\). Since \(g\) is surjective, for every \(x_i\) there exists an \(m_i \in M_2\) such that \(g(m_i) = x_i\), thus define \(h:M_3\mapsto M_2\) such that \(\forall i \in \mathcal{I}:h(x_i) = m_i\), and since for every \(x \in M_3\) can be written as \(x = \sum_j \alpha_{i_j}x_{i_j}\), define \(h(x) = \sum_j \alpha_{i_j}h(x_{i_j})\). It follows that \(h\) is indeed an \(R\)-homomorphism by construction, and that</p> \[g(h(x)) = g(\sum_j \alpha_{i_j}h(x_{i_j}))=\sum_j \alpha_{i_j}g(h(x_{i_j}))=\sum_j \alpha_{i_j}x_{i_j} = x\] <p>thus \(g \circ h = \text{id}_{M_3}\), and by the previous Lemma we conclude that the sequence is split exact.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="the-main-result">The main result</h2> <p>Our strategy here is relatively simple: we will proceed via induction on the rank of the \(R\)-module. We will try to express any submodule as a direct sum of free modules with limited rank, and then conclude that the submodule itself also must have limited rank. This is where the exact sequences will come into play: we will build a natural exact sequence from the fact that our module is free with finite rank, and from this we’ll use the previous lemmas together with induction to conclude the desired result.</p> <p>Recall that any \(R\)-module over a unitary commutative ring with finite basis has the rank invariant property, i.e., any basis must have the same size. We now prove the main result.</p> <p><strong>Theorem</strong>: Let \(D\) be a principal ideal domain and let \(M\) be a free \(D\)-module with finite basis and rank \(n\), then any \(D\)-submodule is also free with rank at most \(n\).</p> <p><strong>Proof:</strong> The proof will be by induction on \(n\). If \(n = 1\), then \(M \cong D\), and since \(D\) is a PID, it means that any \(D\)-submodule of \(D\) is an ideal of \(D\), thus being generated by at most one element, i.e., each \(D\)-submodule is free and with rank at most \(1\). Now assume that if the rank of \(M\) is strictly smaller than \(n\), then any \(D\)-submodule is also free with rank at most the rank of \(M\).</p> <p>Let the rank of \(M\) be \(n\). It follows that \(M \cong D^{(n)}\), i.e., \(M\) is isomorphic to the direct sum of \(D\) with itself \(n\) times. Let such isomorphism be denoted by \(\phi\), and let \(\pi_1:D^{(n)} \mapsto D,\pi_1((r_1,...,r_n))=r_1\) be the projection onto \(D\) w.r.t. the first element of the \(n\)-tuple. Let \(\{x_1,...,x_n\}\) be a basis for \(M\), and assume without loss of generality that \(\phi(x_i)=e_i\), where \(e_i\) denotes the \(n\)-tuple with \(1\) on the \(i\)-th position and \(0\) elsewhere. Define \(\psi:M\mapsto D,\psi = \pi_1 \circ \phi\), and let \(N \leq M\) be a \(D\)-submodule and \(L = \langle x_2,...,x_n\rangle\) be the \(D\)-submodule of \(M\) generated by the elements \(x_2,...,x_n\).</p> <p>Let \(\tau\) denote the restriction of \(\psi\) onto \(N\), and note that \(\text{Im}(\tau)\leq D\) by definition, thus by the induction hypothesis \(\text{Im}(\tau)\) is a free \(D\)-submodule of \(D\) with rank at most \(1\) (since \(D\) has rank \(1\)). Also, note that \(\ker(\tau)\leq L\), since \(\ker(\psi) = L\), and since \(L\) is a free \(D\)-module of rank \(n-1\), it follows that the rank of \(\ker(\tau)\) is at most \(n-1\). Now consider the following diagram:</p> \[0 \mapsto \ker(\tau) \mapsto N \mapsto_{\tau} \text{Im}(\tau) \mapsto 0\] <p>Note that the diagram is clearly an exact sequence, where the mapping between \(\ker(\tau)\) and \(N\) is merely the identity in \(N\). Also, we’ve concluded that \(\text{Im}(\tau)\) is a free \(D\)-module, thus by Lemma \(2\) it follows that the sequence is split exact, and then by Lemma \(1\) it follows that \(N \cong \ker(\tau) \oplus \text{Im}(\tau)\). However \(\ker(\tau) \oplus \text{Im}(\tau)\) is a free \(D\)-module of rank at most \(n\), and thus \(N\) is a free \(D\)-module of rank at most \(n\), which concludes the proof.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>I found the proof of the main theorem to be extremely interesting: although the exact sequence that we create arises naturally, having the idea of building such sequence in the first place is really clever, and in first sight the properties of split exact sequences given by Lemmas 1 and 2 are not at all obvious. However, after trying to prove the theorem by myself, I believe that the meaning of these results becomes much more clear and natural.</p> <p>This result also shows how much more complicated things can become once one generalizes the notion of a vector space. When dealing with finite dimensional vector spaces over a field, the main theorem is way simpler to prove: it is easy to show that any linearly independent set of vectors can either be extended to a bigger linearly independent set of vectors or it already is a basis (in order to prove this one needs invertibility, i.e., the ring must be a division ring). From this, given an arbitrary subspace, any set of linearly independent vectors must be smaller than the dimension of the entire space, and thus we can choose an arbitrary non-zero vector of the subspace and extend it to a basis of the subspace, hence concluding that the rank of the subspace is at most the rank of the space.</p>]]></content><author><name></name></author><category term="math"/><category term="rings-and-modules"/><summary type="html"><![CDATA[another interesting result from the course on rings and modules]]></summary></entry><entry><title type="html">when will an endomorphism ring be a field?</title><link href="https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings/" rel="alternate" type="text/html" title="when will an endomorphism ring be a field?"/><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings</id><content type="html" xml:base="https://henriqueassumpcao.github.io/blog/2023/endomorphism_rings/"><![CDATA[<p>I’m currently taking a course on Rings and Modules at UFMG, and this week I came across an interesting result that I’ve decided to share here.</p> <p>Let \(R\) be a ring with unity. We wish to show that if \(R\) is commutative and if \(M\) is a simple \(R\)-module, then the ring of \(R\)-endomorphisms \(\text{End}_R(M)\) is a field.</p> <h2 id="isomorphisms-between-rings-of-endomorphisms">Isomorphisms between rings of endomorphisms</h2> <p>We will first prove an auxiliary result that turns out to be quite useful.</p> <p><strong>Lemma 1:</strong> Let \(R\) be a unitary ring, and let \(M,N\) be two isomorphic \(R\)-modules. Then \(\text{End}_R(M)\) and \(\text{End}_R(N)\) are isomorphic as rings.</p> <p><strong>Proof:</strong></p> <p>Let \(\phi:M\mapsto N\) be the \(R\)-isomorphism between \(M\) and \(N\). Define \(\psi:\text{End}_R(M)\mapsto \text{End}_R(N)\), where \(\psi(f)(n) = \phi(f(\phi^{-1}(n)))\), for all \(f \in \text{End}_R(M),n \in N\). We claim that \(\psi\) is a ring isomorphism.</p> <p>If \(f,g \in \text{End}_R(M)\), then</p> \[\psi(f+g)(n)=\phi((f+g)(\phi^{-1}(n)))\] \[=\phi(f(\phi^{-1}(n))+g(\phi^{-1}(n)))\] \[= \phi(f(\phi^{-1}(n)))+\phi(g(\phi^{-1}(n)))\] \[= \psi(f)(n) + \psi(g)(n),\forall n \in N\] <p>since \(\phi\) is itself an endomorphism, and thus additive over \(M\).</p> <p>Also</p> \[\psi(f\circ g)(n) = \phi(f(g(\phi^{-1}(n))))\] \[=\phi(f(\phi^{-1}(\phi(g(\phi^{-1}(n))))))\] \[=\phi(f(\phi^{-1}(\psi(g)(n))))\] \[=\psi(f)(\psi(g)(n)) = (\psi(f) \circ \psi(g))(n),\forall n \in N\] <p>therefore \(\psi\) is a ring homomorphism.</p> <p>If \(f \in \ker(\psi)\), we have that \(\forall n \in N: \psi(f)(n) = 0 \Rightarrow \phi(f(\phi^{-1}(n))) = 0\). Since \(\phi\) is a \(R\)-module isomorphism, this implies that \(\forall n \in N: f(\phi^{-1}(n)) = 0\), but again \(\phi\) is an isomorphism, thus its inverse is also surjective, which implies that \(\forall m \in M: f(m) = 0\), and so \(f = 0 \Rightarrow \ker(\psi)=\{0\}\), i.e., \(\psi\) is injective.</p> <p>Given \(g \in \text{End}_R(N)\), define \(f(m) = \phi^{-1}(g(\phi(m)))\). Note that \(f\) is an endomorphism, since \(\phi\) and \(g\) both are. Also, note that</p> \[\psi(f)(n) = \phi(\phi^{-1}(g(\phi(\phi^{-1}(n))))) = g(n),\forall n \in N\] <p>thus \(\psi(f) = g\), and so \(\psi\) is surjective. The previous points show that \(\psi\) is indeed a ring isomorphism, which concludes the proof.</p> <p>\(\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\)</p> <h2 id="endomorphisms-of-commutative-rings">Endomorphisms of commutative rings</h2> <p>We now look at the ring of \(R\)-homomorphisms of a commutative unitary ring \(R\).</p> <p><strong>Lemma 2:</strong> Let \(R\) be a commutative ring with unity. The following statements hold:</p> <ol> <li>The rings \(R\) and \(\text{End}_R(R)\) are isomorphic via \(\phi:\text{End}_R(R)\mapsto R,\phi(f)=f(1),\forall f \in \text{End}_R(R)\).</li> <li>If \(I\) is an ideal of \(R\), then \(R/I\) is isomorphic to \(\text{End}_R(R/I)\).</li> </ol> <p><strong>Proof:</strong></p> <p>For the first statement, let \(f,g \in \text{End}_R(R)\), and note that</p> \[\phi(f+g)=(f+g)(1)=f(1)+g(1)=\phi(f)+\phi(g)\] \[\phi(f\circ g) = f(g(1)) = f(1\cdot g(1))=f(1)g(1)=\phi(f)\cdot\phi(g)\] <p>thus \(\phi\) is a ring homomorphism. If \(f \in \ker(\phi)\), we have that \(f(1) = 0\), but note that \(\forall a \in R:f(a) = a\cdot f(1)\), since \(f\) is a \(R\)-homomorphism, thus if \(f(1)=0\), this implies that \(f = 0\), and so \(\phi\) is injective. Moreover, given any \(a \in R\), we can define \(f(1)=a\), and \(f(b)=b\cdot a\), which yields that \(\phi(f)=a\) and that \(f\) is a \(R\)-homomorphism. Therefore \(\phi\) is a ring isomorphism.</p> <p>For the second statement, note that since \(R\) is commutative and \(I\) is a bilateral ideal of \(R\), \(R/I\) is a ring, and thus \(R/I\) can be viewed as an \(R/I\)-module. On the other hand, since \(I\) is an ideal and \(I\) is an \(R\)-module, \(R/I\) is also an \(R\)-module. Thus, if \(f \in \text{End}_{R/I}(R/I)\), note that</p> \[f(a \cdot (b+I))=f((a+I)(b+I)) = (a+I)f(b+I)=a\cdot f(b+I)\] <p>thus \(f \in \text{End}_{R}(R/I)\). Conversely, if \(f \in \text{End}_{R}(R/I)\), we have that</p> \[f((a+I)(b+I))=f(ab+I)=f(a(b+I))=af(b+I)=(a+I)f(b+I)\] <p>and so \(f \in \text{End}_{R}(R/I)\), implying that \(\text{End}_{R/I}(R/I) = \text{End}_{R}(R/I)\). Using this fact and the previous result, we conclude that \(R/I\) and \(\text{End}_{R}(R/I)\) are isomorphic rings.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="putting-it-all-together">Putting it all together</h2> <p>Recall that an \(R\)-module \(M\) is called simple if it has no proper non-trivial submodules, i.e., its only submodules are \(\{0\}\) and \(M\). We know that this is equivalent to two things: first that \(M\) must be cyclic, and every non-zero element of \(M\) must be a generator, and second that \(M\) is \(R\)-isomorphic to \(R/I\), where \(I\) is some maximal left-ideal of \(R\). We are now ready to prove our main result.</p> <p><strong>Theorem:</strong> Let \(R\) be a commutative ring with unity, and \(M\) be a simple \(R\)-module. Then \(\text{End}_{R}(M)\) is a field.</p> <p><strong>Proof:</strong></p> <p>Since \(M\) is simple, it follows that it is \(R\)-isomorphic to \(R/I\), where \(I\) is a maximal left-ideal of \(R\), but since \(R\) is commutative, this implies that \(I\) is a maximal bilateral ideal. From Lemma 1, it follows that \(\text{End}_{R}(M)\) and \(\text{End}_{R}(R/I)\) are isomorphic rings, and from Lemma 2 it follows that \(\text{End}_{R}(R/I)\) is isomorphic to \(R/I\) as rings, so \(\text{End}_{R}(M)\) is isomorphic to \(R/I\) as rings. However, note that since \(I\) is a maximal bilateral ideal, \(R/I\) is a field, and thus \(\text{End}_{R}(M)\) is itself a field.</p> \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\blacksquare\] <h2 id="comments">Comments</h2> <p>The proof of this result is quite simple due to it being mainly a series of straightforward algebraic manipulations. However, I still think that it portrays some interesting techniques.</p> <p>Lemma 1 essentially provides a way of converting a module homomorphism between two modules to a ring homomorphism between two rings that are closely related to the original modules. This surely has limited applications, however in our case it is precisely what we needed: we start by assuming that \(M\) is simple, and thus it is isomorphic to \(R/I\) as a module, however due to the fact that \(I\) is maximal we also know that \(R/I\) is a field. We then need a way of connecting this module isomorphism to a ring isomorphism, and this is precisely what Lemma 1 does. Lemma 2 on the other hand is just a bundle of simple observations that follow from basic properties of unitary commutative rings, and so it itself isn’t of much interest, however combined with Lemma 1 it yields the desired result. As for the main theorem itself, it yields a more powerful version of Schur’s Lemma: function composition for simple modules over commutative rings is itself commutative.</p>]]></content><author><name></name></author><category term="math"/><category term="rings-and-modules"/><summary type="html"><![CDATA[an interesting result from the course on rings and modules]]></summary></entry></feed>